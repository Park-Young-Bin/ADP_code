{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ddf1a3",
   "metadata": {},
   "source": [
    "[깃허브 주소](https://github.com/ADPclass/ADP_book_ver01/tree/main/%EC%B5%9C%EC%8B%A0%20%EA%B8%B0%EC%B6%9C%EB%8F%99%ED%98%95%20%EB%AA%A8%EC%9D%98%EA%B3%A0%EC%82%AC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c7418",
   "metadata": {},
   "source": [
    "## 머신러닝\n",
    "### 1. 데이터 전처리 및 군집생성\n",
    "#### (1) 결측치를 확인하고 결측치를 제거하시오(EDA).\n",
    "#### (2) 이상치 제거하는 방법을 설명하고, 이상치 제거하고 난 결과를 통계적으로 나타내시요.\n",
    "#### (3) 전처리한 데이터로 Kmeans, DBSCAN 등 방법으로 군집을 생성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d4a94c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>NumDealsPurchases</th>\n",
       "      <th>NumWebPurchases</th>\n",
       "      <th>NumCatalogPurchases</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5524</td>\n",
       "      <td>1957</td>\n",
       "      <td>Single</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>635</td>\n",
       "      <td>88</td>\n",
       "      <td>546</td>\n",
       "      <td>172</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2174</td>\n",
       "      <td>1954</td>\n",
       "      <td>Single</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4141</td>\n",
       "      <td>1965</td>\n",
       "      <td>Together</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>49</td>\n",
       "      <td>127</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6182</td>\n",
       "      <td>1984</td>\n",
       "      <td>Together</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5324</td>\n",
       "      <td>1981</td>\n",
       "      <td>Married</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>43</td>\n",
       "      <td>118</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  Year_Birth Marital_Status   Income  Kidhome  Teenhome  MntWines  \\\n",
       "0  5524        1957         Single  58138.0        0         0       635   \n",
       "1  2174        1954         Single  46344.0        1         1        11   \n",
       "2  4141        1965       Together  71613.0        0         0       426   \n",
       "3  6182        1984       Together  26646.0        1         0        11   \n",
       "4  5324        1981        Married  58293.0        1         0       173   \n",
       "\n",
       "   MntFruits  MntMeatProducts  MntFishProducts  MntSweetProducts  \\\n",
       "0         88              546              172                88   \n",
       "1          1                6                2                 1   \n",
       "2         49              127              111                21   \n",
       "3          4               20               10                 3   \n",
       "4         43              118               46                27   \n",
       "\n",
       "   NumDealsPurchases  NumWebPurchases  NumCatalogPurchases  NumStorePurchases  \\\n",
       "0                  3                8                   10                  4   \n",
       "1                  2                1                    1                  2   \n",
       "2                  1                8                    2                 10   \n",
       "3                  2                2                    0                  4   \n",
       "4                  5                5                    3                  6   \n",
       "\n",
       "   NumWebVisitsMonth  \n",
       "0                  7  \n",
       "1                  5  \n",
       "2                  4  \n",
       "3                  6  \n",
       "4                  5  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/26_problem1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2da852",
   "metadata": {},
   "source": [
    "#### (1) 결측치를 확인하고 결측치를 제거하시오(EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a66104b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2240 entries, 0 to 2239\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   ID                   2240 non-null   int64  \n",
      " 1   Year_Birth           2240 non-null   int64  \n",
      " 2   Marital_Status       2240 non-null   object \n",
      " 3   Income               2216 non-null   float64\n",
      " 4   Kidhome              2240 non-null   int64  \n",
      " 5   Teenhome             2240 non-null   int64  \n",
      " 6   MntWines             2240 non-null   int64  \n",
      " 7   MntFruits            2240 non-null   int64  \n",
      " 8   MntMeatProducts      2240 non-null   int64  \n",
      " 9   MntFishProducts      2240 non-null   int64  \n",
      " 10  MntSweetProducts     2240 non-null   int64  \n",
      " 11  NumDealsPurchases    2240 non-null   int64  \n",
      " 12  NumWebPurchases      2240 non-null   int64  \n",
      " 13  NumCatalogPurchases  2240 non-null   int64  \n",
      " 14  NumStorePurchases    2240 non-null   int64  \n",
      " 15  NumWebVisitsMonth    2240 non-null   int64  \n",
      "dtypes: float64(1), int64(14), object(1)\n",
      "memory usage: 280.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ded429c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                      0\n",
       "Year_Birth              0\n",
       "Marital_Status          0\n",
       "Income                 24\n",
       "Kidhome                 0\n",
       "Teenhome                0\n",
       "MntWines                0\n",
       "MntFruits               0\n",
       "MntMeatProducts         0\n",
       "MntFishProducts         0\n",
       "MntSweetProducts        0\n",
       "NumDealsPurchases       0\n",
       "NumWebPurchases         0\n",
       "NumCatalogPurchases     0\n",
       "NumStorePurchases       0\n",
       "NumWebVisitsMonth       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540fbbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marital_Status\n",
       "Absurd      72365.5\n",
       "Alone       35860.0\n",
       "Divorced    52683.0\n",
       "Married     51876.0\n",
       "Single      48904.0\n",
       "Together    51369.0\n",
       "Widow       56551.0\n",
       "YOLO        48432.0\n",
       "Name: Income, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Marital_Status']).median()['Income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43239c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>NumDealsPurchases</th>\n",
       "      <th>NumWebPurchases</th>\n",
       "      <th>NumCatalogPurchases</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5524</td>\n",
       "      <td>1957</td>\n",
       "      <td>Single</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>635</td>\n",
       "      <td>88</td>\n",
       "      <td>546</td>\n",
       "      <td>172</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2174</td>\n",
       "      <td>1954</td>\n",
       "      <td>Single</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4141</td>\n",
       "      <td>1965</td>\n",
       "      <td>Together</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>49</td>\n",
       "      <td>127</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6182</td>\n",
       "      <td>1984</td>\n",
       "      <td>Together</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5324</td>\n",
       "      <td>1981</td>\n",
       "      <td>Married</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>43</td>\n",
       "      <td>118</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>10870</td>\n",
       "      <td>1967</td>\n",
       "      <td>Married</td>\n",
       "      <td>61223.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>709</td>\n",
       "      <td>43</td>\n",
       "      <td>182</td>\n",
       "      <td>42</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>4001</td>\n",
       "      <td>1946</td>\n",
       "      <td>Together</td>\n",
       "      <td>64014.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>406</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>7270</td>\n",
       "      <td>1981</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>56981.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>908</td>\n",
       "      <td>48</td>\n",
       "      <td>217</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>8235</td>\n",
       "      <td>1956</td>\n",
       "      <td>Together</td>\n",
       "      <td>69245.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>428</td>\n",
       "      <td>30</td>\n",
       "      <td>214</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>9405</td>\n",
       "      <td>1954</td>\n",
       "      <td>Married</td>\n",
       "      <td>52869.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2240 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Year_Birth Marital_Status   Income  Kidhome  Teenhome  MntWines  \\\n",
       "0      5524        1957         Single  58138.0        0         0       635   \n",
       "1      2174        1954         Single  46344.0        1         1        11   \n",
       "2      4141        1965       Together  71613.0        0         0       426   \n",
       "3      6182        1984       Together  26646.0        1         0        11   \n",
       "4      5324        1981        Married  58293.0        1         0       173   \n",
       "...     ...         ...            ...      ...      ...       ...       ...   \n",
       "2235  10870        1967        Married  61223.0        0         1       709   \n",
       "2236   4001        1946       Together  64014.0        2         1       406   \n",
       "2237   7270        1981       Divorced  56981.0        0         0       908   \n",
       "2238   8235        1956       Together  69245.0        0         1       428   \n",
       "2239   9405        1954        Married  52869.0        1         1        84   \n",
       "\n",
       "      MntFruits  MntMeatProducts  MntFishProducts  MntSweetProducts  \\\n",
       "0            88              546              172                88   \n",
       "1             1                6                2                 1   \n",
       "2            49              127              111                21   \n",
       "3             4               20               10                 3   \n",
       "4            43              118               46                27   \n",
       "...         ...              ...              ...               ...   \n",
       "2235         43              182               42               118   \n",
       "2236          0               30                0                 0   \n",
       "2237         48              217               32                12   \n",
       "2238         30              214               80                30   \n",
       "2239          3               61                2                 1   \n",
       "\n",
       "      NumDealsPurchases  NumWebPurchases  NumCatalogPurchases  \\\n",
       "0                     3                8                   10   \n",
       "1                     2                1                    1   \n",
       "2                     1                8                    2   \n",
       "3                     2                2                    0   \n",
       "4                     5                5                    3   \n",
       "...                 ...              ...                  ...   \n",
       "2235                  2                9                    3   \n",
       "2236                  7                8                    2   \n",
       "2237                  1                2                    3   \n",
       "2238                  2                6                    5   \n",
       "2239                  3                3                    1   \n",
       "\n",
       "      NumStorePurchases  NumWebVisitsMonth  \n",
       "0                     4                  7  \n",
       "1                     2                  5  \n",
       "2                    10                  4  \n",
       "3                     4                  6  \n",
       "4                     6                  5  \n",
       "...                 ...                ...  \n",
       "2235                  4                  5  \n",
       "2236                  5                  7  \n",
       "2237                 13                  6  \n",
       "2238                 10                  3  \n",
       "2239                  4                  7  \n",
       "\n",
       "[2240 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f91b829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                     0\n",
       "Year_Birth             0\n",
       "Marital_Status         0\n",
       "Income                 0\n",
       "Kidhome                0\n",
       "Teenhome               0\n",
       "MntWines               0\n",
       "MntFruits              0\n",
       "MntMeatProducts        0\n",
       "MntFishProducts        0\n",
       "MntSweetProducts       0\n",
       "NumDealsPurchases      0\n",
       "NumWebPurchases        0\n",
       "NumCatalogPurchases    0\n",
       "NumStorePurchases      0\n",
       "NumWebVisitsMonth      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nonull = df\n",
    "\n",
    "fill_median_func = lambda g: g.fillna(g.median())\n",
    "df_nonull = df_nonull.groupby('Marital_Status').apply(fill_median_func)\n",
    "\n",
    "df_nonull.index = df_nonull.index.droplevel(0)\n",
    "df_nonull.sort_index(inplace=True)\n",
    "\n",
    "df_nonull.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515c9aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                     0\n",
       "Year_Birth             0\n",
       "Marital_Status         0\n",
       "Income                 0\n",
       "Kidhome                0\n",
       "Teenhome               0\n",
       "MntWines               0\n",
       "MntFruits              0\n",
       "MntMeatProducts        0\n",
       "MntFishProducts        0\n",
       "MntSweetProducts       0\n",
       "NumDealsPurchases      0\n",
       "NumWebPurchases        0\n",
       "NumCatalogPurchases    0\n",
       "NumStorePurchases      0\n",
       "NumWebVisitsMonth      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nonull.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f5a859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>NumDealsPurchases</th>\n",
       "      <th>NumWebPurchases</th>\n",
       "      <th>NumCatalogPurchases</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5524</td>\n",
       "      <td>1957</td>\n",
       "      <td>Single</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>635</td>\n",
       "      <td>88</td>\n",
       "      <td>546</td>\n",
       "      <td>172</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2174</td>\n",
       "      <td>1954</td>\n",
       "      <td>Single</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4141</td>\n",
       "      <td>1965</td>\n",
       "      <td>Together</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>49</td>\n",
       "      <td>127</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6182</td>\n",
       "      <td>1984</td>\n",
       "      <td>Together</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5324</td>\n",
       "      <td>1981</td>\n",
       "      <td>Married</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>43</td>\n",
       "      <td>118</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>10870</td>\n",
       "      <td>1967</td>\n",
       "      <td>Married</td>\n",
       "      <td>61223.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>709</td>\n",
       "      <td>43</td>\n",
       "      <td>182</td>\n",
       "      <td>42</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>4001</td>\n",
       "      <td>1946</td>\n",
       "      <td>Together</td>\n",
       "      <td>64014.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>406</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>7270</td>\n",
       "      <td>1981</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>56981.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>908</td>\n",
       "      <td>48</td>\n",
       "      <td>217</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>8235</td>\n",
       "      <td>1956</td>\n",
       "      <td>Together</td>\n",
       "      <td>69245.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>428</td>\n",
       "      <td>30</td>\n",
       "      <td>214</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>9405</td>\n",
       "      <td>1954</td>\n",
       "      <td>Married</td>\n",
       "      <td>52869.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2240 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Year_Birth Marital_Status   Income  Kidhome  Teenhome  MntWines  \\\n",
       "0      5524        1957         Single  58138.0        0         0       635   \n",
       "1      2174        1954         Single  46344.0        1         1        11   \n",
       "2      4141        1965       Together  71613.0        0         0       426   \n",
       "3      6182        1984       Together  26646.0        1         0        11   \n",
       "4      5324        1981        Married  58293.0        1         0       173   \n",
       "...     ...         ...            ...      ...      ...       ...       ...   \n",
       "2235  10870        1967        Married  61223.0        0         1       709   \n",
       "2236   4001        1946       Together  64014.0        2         1       406   \n",
       "2237   7270        1981       Divorced  56981.0        0         0       908   \n",
       "2238   8235        1956       Together  69245.0        0         1       428   \n",
       "2239   9405        1954        Married  52869.0        1         1        84   \n",
       "\n",
       "      MntFruits  MntMeatProducts  MntFishProducts  MntSweetProducts  \\\n",
       "0            88              546              172                88   \n",
       "1             1                6                2                 1   \n",
       "2            49              127              111                21   \n",
       "3             4               20               10                 3   \n",
       "4            43              118               46                27   \n",
       "...         ...              ...              ...               ...   \n",
       "2235         43              182               42               118   \n",
       "2236          0               30                0                 0   \n",
       "2237         48              217               32                12   \n",
       "2238         30              214               80                30   \n",
       "2239          3               61                2                 1   \n",
       "\n",
       "      NumDealsPurchases  NumWebPurchases  NumCatalogPurchases  \\\n",
       "0                     3                8                   10   \n",
       "1                     2                1                    1   \n",
       "2                     1                8                    2   \n",
       "3                     2                2                    0   \n",
       "4                     5                5                    3   \n",
       "...                 ...              ...                  ...   \n",
       "2235                  2                9                    3   \n",
       "2236                  7                8                    2   \n",
       "2237                  1                2                    3   \n",
       "2238                  2                6                    5   \n",
       "2239                  3                3                    1   \n",
       "\n",
       "      NumStorePurchases  NumWebVisitsMonth  \n",
       "0                     4                  7  \n",
       "1                     2                  5  \n",
       "2                    10                  4  \n",
       "3                     4                  6  \n",
       "4                     6                  5  \n",
       "...                 ...                ...  \n",
       "2235                  4                  5  \n",
       "2236                  5                  7  \n",
       "2237                 13                  6  \n",
       "2238                 10                  3  \n",
       "2239                  4                  7  \n",
       "\n",
       "[2240 rows x 16 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nonull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af5f064",
   "metadata": {},
   "source": [
    "[해석]\n",
    "- 결측치는 Income 변수에만 있었고, Marital_Status 별 Income의 중앙값으로 대체하였음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545bdc00",
   "metadata": {},
   "source": [
    "#### (2) 이상치 제거하는 방법을 설명하고, 이상치 제거하고 난 결과를 통계적으로 나타내시요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb1ba75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Year_Birth</td>\n",
       "      <td>1957.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Year_Birth</td>\n",
       "      <td>1954.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Year_Birth</td>\n",
       "      <td>1965.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Year_Birth</td>\n",
       "      <td>1984.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Year_Birth</td>\n",
       "      <td>1981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31355</th>\n",
       "      <td>NumWebVisitsMonth</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31356</th>\n",
       "      <td>NumWebVisitsMonth</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31357</th>\n",
       "      <td>NumWebVisitsMonth</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31358</th>\n",
       "      <td>NumWebVisitsMonth</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31359</th>\n",
       "      <td>NumWebVisitsMonth</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31360 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     col   value\n",
       "0             Year_Birth  1957.0\n",
       "1             Year_Birth  1954.0\n",
       "2             Year_Birth  1965.0\n",
       "3             Year_Birth  1984.0\n",
       "4             Year_Birth  1981.0\n",
       "...                  ...     ...\n",
       "31355  NumWebVisitsMonth     5.0\n",
       "31356  NumWebVisitsMonth     7.0\n",
       "31357  NumWebVisitsMonth     6.0\n",
       "31358  NumWebVisitsMonth     3.0\n",
       "31359  NumWebVisitsMonth     7.0\n",
       "\n",
       "[31360 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 연속형 변수만 선택\n",
    "df_select = df_nonull.drop(columns=['ID', 'Marital_Status'], axis=1)\n",
    "df_melt = pd.melt(df_select, var_name='col', value_name='value')\n",
    "df_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98531a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.boxplot(data=df_melt, x='col', y='value')\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d519698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>NumDealsPurchases</th>\n",
       "      <th>NumWebPurchases</th>\n",
       "      <th>NumCatalogPurchases</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>8475</td>\n",
       "      <td>1973</td>\n",
       "      <td>Married</td>\n",
       "      <td>157243.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1582</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>1503</td>\n",
       "      <td>1976</td>\n",
       "      <td>Together</td>\n",
       "      <td>162397.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>5555</td>\n",
       "      <td>1975</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>153924.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>1501</td>\n",
       "      <td>1982</td>\n",
       "      <td>Married</td>\n",
       "      <td>160803.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>1622</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>5336</td>\n",
       "      <td>1971</td>\n",
       "      <td>Together</td>\n",
       "      <td>157733.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>4931</td>\n",
       "      <td>1977</td>\n",
       "      <td>Together</td>\n",
       "      <td>157146.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1725</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>11181</td>\n",
       "      <td>1949</td>\n",
       "      <td>Married</td>\n",
       "      <td>156924.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>9432</td>\n",
       "      <td>1977</td>\n",
       "      <td>Together</td>\n",
       "      <td>666666.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Year_Birth Marital_Status    Income  Kidhome  Teenhome  MntWines  \\\n",
       "164    8475        1973        Married  157243.0        0         1        20   \n",
       "617    1503        1976       Together  162397.0        1         1        85   \n",
       "655    5555        1975       Divorced  153924.0        0         0         1   \n",
       "687    1501        1982        Married  160803.0        0         0        55   \n",
       "1300   5336        1971       Together  157733.0        1         0        39   \n",
       "1653   4931        1977       Together  157146.0        0         0         1   \n",
       "2132  11181        1949        Married  156924.0        0         0         2   \n",
       "2233   9432        1977       Together  666666.0        1         0         9   \n",
       "\n",
       "      MntFruits  MntMeatProducts  MntFishProducts  MntSweetProducts  \\\n",
       "164           2             1582                1                 2   \n",
       "617           1               16                2                 1   \n",
       "655           1                1                1                 1   \n",
       "687          16             1622               17                 3   \n",
       "1300          1                9                2                 0   \n",
       "1653          0             1725                2                 1   \n",
       "2132          1                2                1                 1   \n",
       "2233         14               18                8                 1   \n",
       "\n",
       "      NumDealsPurchases  NumWebPurchases  NumCatalogPurchases  \\\n",
       "164                  15                0                   22   \n",
       "617                   0                0                    0   \n",
       "655                   0                0                    0   \n",
       "687                  15                0                   28   \n",
       "1300                  0                1                    0   \n",
       "1653                  0                0                   28   \n",
       "2132                  0                0                    0   \n",
       "2233                  4                3                    1   \n",
       "\n",
       "      NumStorePurchases  NumWebVisitsMonth  \n",
       "164                   0                  0  \n",
       "617                   1                  1  \n",
       "655                   0                  0  \n",
       "687                   1                  0  \n",
       "1300                  1                  1  \n",
       "1653                  0                  1  \n",
       "2132                  0                  0  \n",
       "2233                  3                  6  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IQR을 이용한 이상치 탐색 함수\n",
    "def detect_outliers(df=None, column=None, weight=1.5):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    \n",
    "    IQR = Q3-Q1\n",
    "    IQR_weight = IQR * weight\n",
    "    \n",
    "    outlier_idx = df[(df[column] < Q1 - IQR_weight) | (df[column] > Q3 + IQR_weight)].index\n",
    "    \n",
    "    return outlier_idx\n",
    "\n",
    "# IQR로 이상치를찾아 인덱스 저장되어 있음.\n",
    "out_index = detect_outliers(df=df_nonull, column='Income')\n",
    "df_nonull.loc[out_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25916325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>NumDealsPurchases</th>\n",
       "      <th>NumWebPurchases</th>\n",
       "      <th>NumCatalogPurchases</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>8475</td>\n",
       "      <td>1973</td>\n",
       "      <td>Married</td>\n",
       "      <td>157243.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1582</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>1503</td>\n",
       "      <td>1976</td>\n",
       "      <td>Together</td>\n",
       "      <td>162397.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>5555</td>\n",
       "      <td>1975</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>153924.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>1501</td>\n",
       "      <td>1982</td>\n",
       "      <td>Married</td>\n",
       "      <td>160803.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>1622</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>5336</td>\n",
       "      <td>1971</td>\n",
       "      <td>Together</td>\n",
       "      <td>157733.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>4931</td>\n",
       "      <td>1977</td>\n",
       "      <td>Together</td>\n",
       "      <td>157146.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1725</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>11181</td>\n",
       "      <td>1949</td>\n",
       "      <td>Married</td>\n",
       "      <td>156924.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>9432</td>\n",
       "      <td>1977</td>\n",
       "      <td>Together</td>\n",
       "      <td>52231.835268</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Year_Birth Marital_Status         Income  Kidhome  Teenhome  \\\n",
       "164    8475        1973        Married  157243.000000        0         1   \n",
       "617    1503        1976       Together  162397.000000        1         1   \n",
       "655    5555        1975       Divorced  153924.000000        0         0   \n",
       "687    1501        1982        Married  160803.000000        0         0   \n",
       "1300   5336        1971       Together  157733.000000        1         0   \n",
       "1653   4931        1977       Together  157146.000000        0         0   \n",
       "2132  11181        1949        Married  156924.000000        0         0   \n",
       "2233   9432        1977       Together   52231.835268        1         0   \n",
       "\n",
       "      MntWines  MntFruits  MntMeatProducts  MntFishProducts  MntSweetProducts  \\\n",
       "164         20          2             1582                1                 2   \n",
       "617         85          1               16                2                 1   \n",
       "655          1          1                1                1                 1   \n",
       "687         55         16             1622               17                 3   \n",
       "1300        39          1                9                2                 0   \n",
       "1653         1          0             1725                2                 1   \n",
       "2132         2          1                2                1                 1   \n",
       "2233         9         14               18                8                 1   \n",
       "\n",
       "      NumDealsPurchases  NumWebPurchases  NumCatalogPurchases  \\\n",
       "164                  15                0                   22   \n",
       "617                   0                0                    0   \n",
       "655                   0                0                    0   \n",
       "687                  15                0                   28   \n",
       "1300                  0                1                    0   \n",
       "1653                  0                0                   28   \n",
       "2132                  0                0                    0   \n",
       "2233                  4                3                    1   \n",
       "\n",
       "      NumStorePurchases  NumWebVisitsMonth  \n",
       "164                   0                  0  \n",
       "617                   1                  1  \n",
       "655                   0                  0  \n",
       "687                   1                  0  \n",
       "1300                  1                  1  \n",
       "1653                  0                  1  \n",
       "2132                  0                  0  \n",
       "2233                  3                  6  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 666666.0 값만 평균으로 대체\n",
    "df_nonull.loc[out_index[-1], 'Income'] = df_nonull['Income'].mean()\n",
    "df_nonull.loc[out_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc826ae9",
   "metadata": {},
   "source": [
    "[해석]\n",
    "- 컬럼별 상자그림에서 Income에서 이상치를 발견하였기에 해당 이상치를 제거하기 위해 IQR로 이상치를 찾음.\n",
    "- 대부분의 이상치는 고소득층으로 볼 수 있지만 666666.0인 값은 비정상적인 값으로 판단하여 Income의 평균값으로 대체함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c59ac761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 내가 작성한 이상치를 확인하기 위한 상자그림\n",
    "# # 연속형 변수들의 상자그림\n",
    "# fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(16, 9))\n",
    "\n",
    "# features = df_select.columns\n",
    "\n",
    "# for i, feature in zip(range(14), features):\n",
    "#     row = int(i/3)\n",
    "#     col = i%3\n",
    "#     sns.boxplot(data=df_select, x=feature, ax=ax[row][col])\n",
    "#     plt.tight_layout()\n",
    "# #     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c10072a",
   "metadata": {},
   "source": [
    "#### (3) 전처리한 데이터로 Kmeans, DBSCAN 등 방법으로 군집을 생성하세요.\n",
    "[해석]\n",
    "- 추가 전처리 군집분석을 위해서 범주형 변수 Marital_Status 인코딩이 필요함.\n",
    "- 최적의 클러스터 개수를 찾기 위해서 엘보우 기법을 사용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37f67fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>...</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "      <th>Marital_Status_Absurd</th>\n",
       "      <th>Marital_Status_Alone</th>\n",
       "      <th>Marital_Status_Divorced</th>\n",
       "      <th>Marital_Status_Married</th>\n",
       "      <th>Marital_Status_Single</th>\n",
       "      <th>Marital_Status_Together</th>\n",
       "      <th>Marital_Status_Widow</th>\n",
       "      <th>Marital_Status_YOLO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5524</td>\n",
       "      <td>1957</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>635</td>\n",
       "      <td>88</td>\n",
       "      <td>546</td>\n",
       "      <td>172</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2174</td>\n",
       "      <td>1954</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4141</td>\n",
       "      <td>1965</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>49</td>\n",
       "      <td>127</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6182</td>\n",
       "      <td>1984</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5324</td>\n",
       "      <td>1981</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>43</td>\n",
       "      <td>118</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  Year_Birth   Income  Kidhome  Teenhome  MntWines  MntFruits  \\\n",
       "0  5524        1957  58138.0        0         0       635         88   \n",
       "1  2174        1954  46344.0        1         1        11          1   \n",
       "2  4141        1965  71613.0        0         0       426         49   \n",
       "3  6182        1984  26646.0        1         0        11          4   \n",
       "4  5324        1981  58293.0        1         0       173         43   \n",
       "\n",
       "   MntMeatProducts  MntFishProducts  MntSweetProducts  ...  NumStorePurchases  \\\n",
       "0              546              172                88  ...                  4   \n",
       "1                6                2                 1  ...                  2   \n",
       "2              127              111                21  ...                 10   \n",
       "3               20               10                 3  ...                  4   \n",
       "4              118               46                27  ...                  6   \n",
       "\n",
       "   NumWebVisitsMonth  Marital_Status_Absurd  Marital_Status_Alone  \\\n",
       "0                  7                      0                     0   \n",
       "1                  5                      0                     0   \n",
       "2                  4                      0                     0   \n",
       "3                  6                      0                     0   \n",
       "4                  5                      0                     0   \n",
       "\n",
       "   Marital_Status_Divorced  Marital_Status_Married  Marital_Status_Single  \\\n",
       "0                        0                       0                      1   \n",
       "1                        0                       0                      1   \n",
       "2                        0                       0                      0   \n",
       "3                        0                       0                      0   \n",
       "4                        0                       1                      0   \n",
       "\n",
       "   Marital_Status_Together  Marital_Status_Widow  Marital_Status_YOLO  \n",
       "0                        0                     0                    0  \n",
       "1                        0                     0                    0  \n",
       "2                        1                     0                    0  \n",
       "3                        1                     0                    0  \n",
       "4                        0                     0                    0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "df_dum = pd.get_dummies(data=df_nonull, columns=['Marital_Status'])\n",
    "df_dum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9fd3ad8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2240 entries, 0 to 2239\n",
      "Data columns (total 23 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   ID                       2240 non-null   int64  \n",
      " 1   Year_Birth               2240 non-null   int64  \n",
      " 2   Income                   2240 non-null   float64\n",
      " 3   Kidhome                  2240 non-null   int64  \n",
      " 4   Teenhome                 2240 non-null   int64  \n",
      " 5   MntWines                 2240 non-null   int64  \n",
      " 6   MntFruits                2240 non-null   int64  \n",
      " 7   MntMeatProducts          2240 non-null   int64  \n",
      " 8   MntFishProducts          2240 non-null   int64  \n",
      " 9   MntSweetProducts         2240 non-null   int64  \n",
      " 10  NumDealsPurchases        2240 non-null   int64  \n",
      " 11  NumWebPurchases          2240 non-null   int64  \n",
      " 12  NumCatalogPurchases      2240 non-null   int64  \n",
      " 13  NumStorePurchases        2240 non-null   int64  \n",
      " 14  NumWebVisitsMonth        2240 non-null   int64  \n",
      " 15  Marital_Status_Absurd    2240 non-null   uint8  \n",
      " 16  Marital_Status_Alone     2240 non-null   uint8  \n",
      " 17  Marital_Status_Divorced  2240 non-null   uint8  \n",
      " 18  Marital_Status_Married   2240 non-null   uint8  \n",
      " 19  Marital_Status_Single    2240 non-null   uint8  \n",
      " 20  Marital_Status_Together  2240 non-null   uint8  \n",
      " 21  Marital_Status_Widow     2240 non-null   uint8  \n",
      " 22  Marital_Status_YOLO      2240 non-null   uint8  \n",
      "dtypes: float64(1), int64(14), uint8(8)\n",
      "memory usage: 377.5 KB\n"
     ]
    }
   ],
   "source": [
    "df_dum.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1154a4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1050019735174.8577, 349224130671.3877, 206900018691.13672, 147505610093.37918, 106270577803.29666, 77199699175.02283, 64193413873.960785, 54684392747.79426, 47605950448.42976, 42584533755.853836]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def elbow(X):\n",
    "    sse = []\n",
    "    for i in range(1, 11):\n",
    "        km = KMeans(n_clusters=i, random_state=1)\n",
    "        km.fit(X)\n",
    "        sse.append(km.inertia_)\n",
    "        \n",
    "    plt.plot(range(1, 11), sse, marker='o')\n",
    "    plt.xlabel('The Number of Clusters')\n",
    "    plt.ylabel('SSE')\n",
    "    print(sse)\n",
    "    \n",
    "elbow(df_dum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a423d7e",
   "metadata": {},
   "source": [
    "[해석]\n",
    "- 시각화 결과, 최적의 클러스터 개수는 3으로 정할 수 있고, 3개의 군집으로 나눌 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74dba387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>NumDealsPurchases</th>\n",
       "      <th>NumWebPurchases</th>\n",
       "      <th>NumCatalogPurchases</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5524</td>\n",
       "      <td>1957</td>\n",
       "      <td>Single</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>635</td>\n",
       "      <td>88</td>\n",
       "      <td>546</td>\n",
       "      <td>172</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2174</td>\n",
       "      <td>1954</td>\n",
       "      <td>Single</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4141</td>\n",
       "      <td>1965</td>\n",
       "      <td>Together</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>49</td>\n",
       "      <td>127</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6182</td>\n",
       "      <td>1984</td>\n",
       "      <td>Together</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5324</td>\n",
       "      <td>1981</td>\n",
       "      <td>Married</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>43</td>\n",
       "      <td>118</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>10870</td>\n",
       "      <td>1967</td>\n",
       "      <td>Married</td>\n",
       "      <td>61223.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>709</td>\n",
       "      <td>43</td>\n",
       "      <td>182</td>\n",
       "      <td>42</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>4001</td>\n",
       "      <td>1946</td>\n",
       "      <td>Together</td>\n",
       "      <td>64014.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>406</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>7270</td>\n",
       "      <td>1981</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>56981.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>908</td>\n",
       "      <td>48</td>\n",
       "      <td>217</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>8235</td>\n",
       "      <td>1956</td>\n",
       "      <td>Together</td>\n",
       "      <td>69245.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>428</td>\n",
       "      <td>30</td>\n",
       "      <td>214</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>9405</td>\n",
       "      <td>1954</td>\n",
       "      <td>Married</td>\n",
       "      <td>52869.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2240 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Year_Birth Marital_Status   Income  Kidhome  Teenhome  MntWines  \\\n",
       "0      5524        1957         Single  58138.0        0         0       635   \n",
       "1      2174        1954         Single  46344.0        1         1        11   \n",
       "2      4141        1965       Together  71613.0        0         0       426   \n",
       "3      6182        1984       Together  26646.0        1         0        11   \n",
       "4      5324        1981        Married  58293.0        1         0       173   \n",
       "...     ...         ...            ...      ...      ...       ...       ...   \n",
       "2235  10870        1967        Married  61223.0        0         1       709   \n",
       "2236   4001        1946       Together  64014.0        2         1       406   \n",
       "2237   7270        1981       Divorced  56981.0        0         0       908   \n",
       "2238   8235        1956       Together  69245.0        0         1       428   \n",
       "2239   9405        1954        Married  52869.0        1         1        84   \n",
       "\n",
       "      MntFruits  MntMeatProducts  MntFishProducts  MntSweetProducts  \\\n",
       "0            88              546              172                88   \n",
       "1             1                6                2                 1   \n",
       "2            49              127              111                21   \n",
       "3             4               20               10                 3   \n",
       "4            43              118               46                27   \n",
       "...         ...              ...              ...               ...   \n",
       "2235         43              182               42               118   \n",
       "2236          0               30                0                 0   \n",
       "2237         48              217               32                12   \n",
       "2238         30              214               80                30   \n",
       "2239          3               61                2                 1   \n",
       "\n",
       "      NumDealsPurchases  NumWebPurchases  NumCatalogPurchases  \\\n",
       "0                     3                8                   10   \n",
       "1                     2                1                    1   \n",
       "2                     1                8                    2   \n",
       "3                     2                2                    0   \n",
       "4                     5                5                    3   \n",
       "...                 ...              ...                  ...   \n",
       "2235                  2                9                    3   \n",
       "2236                  7                8                    2   \n",
       "2237                  1                2                    3   \n",
       "2238                  2                6                    5   \n",
       "2239                  3                3                    1   \n",
       "\n",
       "      NumStorePurchases  NumWebVisitsMonth  cluster  \n",
       "0                     4                  7        2  \n",
       "1                     2                  5        2  \n",
       "2                    10                  4        0  \n",
       "3                     4                  6        1  \n",
       "4                     6                  5        2  \n",
       "...                 ...                ...      ...  \n",
       "2235                  4                  5        2  \n",
       "2236                  5                  7        2  \n",
       "2237                 13                  6        2  \n",
       "2238                 10                  3        0  \n",
       "2239                  4                  7        2  \n",
       "\n",
       "[2240 rows x 17 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최적의 k로 k-means 군집화 실행\n",
    "km = KMeans(n_clusters=3, random_state=1)\n",
    "km.fit(df_dum)\n",
    "\n",
    "# 할당된 군집을 credit 데이터에 추가\n",
    "new_labels = km.labels_\n",
    "df_nonull['cluster'] = new_labels\n",
    "df_nonull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a2d3f",
   "metadata": {},
   "source": [
    "[해석]\n",
    "- 최종적으로, 원핫인코딩이 되어있는 전처리 데이터가 아닌, 원본 데이터에 군집의 라벨(0,1,2)을 부여하여 해당 군집들의 특성을 분석할 수 있는 전처리를 완료함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2828d53b",
   "metadata": {},
   "source": [
    "### 2. 군집분석\n",
    "#### (1) 위에서 생성한 군집들의 특성을 분석하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "845f84b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    815\n",
       "1    737\n",
       "0    688\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nonull.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "172aa214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>ID</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>NumDealsPurchases</th>\n",
       "      <th>NumWebPurchases</th>\n",
       "      <th>NumCatalogPurchases</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5763.226744</td>\n",
       "      <td>1967.434593</td>\n",
       "      <td>76949.853198</td>\n",
       "      <td>0.085756</td>\n",
       "      <td>0.350291</td>\n",
       "      <td>616.405523</td>\n",
       "      <td>56.969477</td>\n",
       "      <td>397.324128</td>\n",
       "      <td>82.802326</td>\n",
       "      <td>60.180233</td>\n",
       "      <td>1.609012</td>\n",
       "      <td>5.402616</td>\n",
       "      <td>5.452035</td>\n",
       "      <td>8.405523</td>\n",
       "      <td>3.159884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5672.206242</td>\n",
       "      <td>1973.099050</td>\n",
       "      <td>28283.426052</td>\n",
       "      <td>0.810041</td>\n",
       "      <td>0.309362</td>\n",
       "      <td>30.221167</td>\n",
       "      <td>6.016282</td>\n",
       "      <td>25.606513</td>\n",
       "      <td>9.115332</td>\n",
       "      <td>6.065129</td>\n",
       "      <td>2.139756</td>\n",
       "      <td>2.154681</td>\n",
       "      <td>0.527815</td>\n",
       "      <td>3.078697</td>\n",
       "      <td>6.907734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5375.364417</td>\n",
       "      <td>1966.080982</td>\n",
       "      <td>52268.089368</td>\n",
       "      <td>0.415951</td>\n",
       "      <td>0.815951</td>\n",
       "      <td>287.676074</td>\n",
       "      <td>18.758282</td>\n",
       "      <td>100.290798</td>\n",
       "      <td>24.995092</td>\n",
       "      <td>18.094479</td>\n",
       "      <td>3.096933</td>\n",
       "      <td>4.717791</td>\n",
       "      <td>2.236810</td>\n",
       "      <td>6.034356</td>\n",
       "      <td>5.698160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster           ID   Year_Birth        Income   Kidhome  Teenhome  \\\n",
       "0        0  5763.226744  1967.434593  76949.853198  0.085756  0.350291   \n",
       "1        1  5672.206242  1973.099050  28283.426052  0.810041  0.309362   \n",
       "2        2  5375.364417  1966.080982  52268.089368  0.415951  0.815951   \n",
       "\n",
       "     MntWines  MntFruits  MntMeatProducts  MntFishProducts  MntSweetProducts  \\\n",
       "0  616.405523  56.969477       397.324128        82.802326         60.180233   \n",
       "1   30.221167   6.016282        25.606513         9.115332          6.065129   \n",
       "2  287.676074  18.758282       100.290798        24.995092         18.094479   \n",
       "\n",
       "   NumDealsPurchases  NumWebPurchases  NumCatalogPurchases  NumStorePurchases  \\\n",
       "0           1.609012         5.402616             5.452035           8.405523   \n",
       "1           2.139756         2.154681             0.527815           3.078697   \n",
       "2           3.096933         4.717791             2.236810           6.034356   \n",
       "\n",
       "   NumWebVisitsMonth  \n",
       "0           3.159884  \n",
       "1           6.907734  \n",
       "2           5.698160  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_mean = df_nonull.groupby('cluster').mean()\n",
    "group_mean.reset_index(inplace=True)\n",
    "group_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "086db21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1967.434593</td>\n",
       "      <td>76949.853198</td>\n",
       "      <td>0.085756</td>\n",
       "      <td>0.350291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1973.099050</td>\n",
       "      <td>28283.426052</td>\n",
       "      <td>0.810041</td>\n",
       "      <td>0.309362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1966.080982</td>\n",
       "      <td>52268.089368</td>\n",
       "      <td>0.415951</td>\n",
       "      <td>0.815951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year_Birth        Income   Kidhome  Teenhome\n",
       "0  1967.434593  76949.853198  0.085756  0.350291\n",
       "1  1973.099050  28283.426052  0.810041  0.309362\n",
       "2  1966.080982  52268.089368  0.415951  0.815951"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 고객 정보 데이터\n",
    "group_mean[['Year_Birth', 'Income', 'Kidhome', 'Teenhome']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e7fa2",
   "metadata": {},
   "source": [
    "[해석]\n",
    "- 그룹 0: 평균 1967년생으로 Income이 가장 높은 집단, 어린아이가 집에 있을 확률이 낮고, 10대 청소년도 약 0.35로 높은 편이 아님.\n",
    "- 그룹 1: 평균 1973년생으로 Income은 세 집단에서 가장 낮음, 어린 아이가 집에 있을 확률이 높음.\n",
    "- 그룹 2: 평균 1966년생으로 그룹 0과 비슷하며, Income은 두 번째로 높음. 10대 청소년이 집이 있을 확률이 0.82로 가장 높음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f95bc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>616.405523</td>\n",
       "      <td>56.969477</td>\n",
       "      <td>397.324128</td>\n",
       "      <td>82.802326</td>\n",
       "      <td>60.180233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.221167</td>\n",
       "      <td>6.016282</td>\n",
       "      <td>25.606513</td>\n",
       "      <td>9.115332</td>\n",
       "      <td>6.065129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287.676074</td>\n",
       "      <td>18.758282</td>\n",
       "      <td>100.290798</td>\n",
       "      <td>24.995092</td>\n",
       "      <td>18.094479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           MntWines  MntFruits  MntMeatProducts  MntFishProducts  \\\n",
       "cluster                                                            \n",
       "0        616.405523  56.969477       397.324128        82.802326   \n",
       "1         30.221167   6.016282        25.606513         9.115332   \n",
       "2        287.676074  18.758282       100.290798        24.995092   \n",
       "\n",
       "         MntSweetProducts  \n",
       "cluster                    \n",
       "0               60.180233  \n",
       "1                6.065129  \n",
       "2               18.094479  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 소비 제품 데이터\n",
    "Mnt_group=group_mean[['cluster', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']]\n",
    "Mnt_group.set_index('cluster', inplace=True)\n",
    "Mnt_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f7d3a",
   "metadata": {},
   "source": [
    "[해석]\n",
    "- 제품별 구매 수가 다르므로 한 번에 해석하기가 어려워, 전체 구매 수로 나누어 비율을 계산함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3cf2204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1213.68168605],\n",
       "       [  77.02442334],\n",
       "       [ 449.81472393]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cluster별 전체 구매 수\n",
    "sum_group_Mnt = np.array(Mnt_group.sum(axis=1)) # 같은 행끼리의 합 계산\n",
    "sum_group_Mnt.reshape(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa909cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.507881</td>\n",
       "      <td>0.046939</td>\n",
       "      <td>0.327371</td>\n",
       "      <td>0.068224</td>\n",
       "      <td>0.049585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.392358</td>\n",
       "      <td>0.078109</td>\n",
       "      <td>0.332447</td>\n",
       "      <td>0.118343</td>\n",
       "      <td>0.078743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.639543</td>\n",
       "      <td>0.041702</td>\n",
       "      <td>0.222960</td>\n",
       "      <td>0.055568</td>\n",
       "      <td>0.040227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MntWines  MntFruits  MntMeatProducts  MntFishProducts  \\\n",
       "cluster                                                          \n",
       "0        0.507881   0.046939         0.327371         0.068224   \n",
       "1        0.392358   0.078109         0.332447         0.118343   \n",
       "2        0.639543   0.041702         0.222960         0.055568   \n",
       "\n",
       "         MntSweetProducts  \n",
       "cluster                    \n",
       "0                0.049585  \n",
       "1                0.078743  \n",
       "2                0.040227  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cluster별 제품 구매수의 비율\n",
    "Mnt_group / sum_group_Mnt.reshape(3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04fba59",
   "metadata": {},
   "source": [
    "[해석]\n",
    "- 그룹 0: 평균 연령 1967년생, 소득이 가장 낲은 집단으로 전체 구매 수 1214로 가장 높고, 그중 와인의 구매가 가장 많음.\n",
    "- 그룹 1: 평균 연령이 가장 낮고(1973), 전체 구매 수가 77회로 가장 낮음. 타 그룹과 비교했을 때, 고기, 과자, 과일의 구매 비율이 가장 높음.\n",
    "- 그룹 2: 평균 연령이 1966년생이고, 소득과 소비가 중간임. 타 그룹과 비교했을 때, 와인의 구매율이 가장 높음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5334bfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumDealsPurchases</th>\n",
       "      <th>NumWebPurchases</th>\n",
       "      <th>NumCatalogPurchases</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.609012</td>\n",
       "      <td>5.402616</td>\n",
       "      <td>5.452035</td>\n",
       "      <td>8.405523</td>\n",
       "      <td>3.159884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.139756</td>\n",
       "      <td>2.154681</td>\n",
       "      <td>0.527815</td>\n",
       "      <td>3.078697</td>\n",
       "      <td>6.907734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.096933</td>\n",
       "      <td>4.717791</td>\n",
       "      <td>2.236810</td>\n",
       "      <td>6.034356</td>\n",
       "      <td>5.698160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumDealsPurchases  NumWebPurchases  NumCatalogPurchases  NumStorePurchases  \\\n",
       "0           1.609012         5.402616             5.452035           8.405523   \n",
       "1           2.139756         2.154681             0.527815           3.078697   \n",
       "2           3.096933         4.717791             2.236810           6.034356   \n",
       "\n",
       "   NumWebVisitsMonth  \n",
       "0           3.159884  \n",
       "1           6.907734  \n",
       "2           5.698160  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구매 채널 데이터\n",
    "group_mean[['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b32b663a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24.02906977],\n",
       "       [14.80868385],\n",
       "       [21.78404908]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cluster별 전체 구매 채널 수\n",
    "sum_group_Num = np.array(group_mean[['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', \n",
    "                                     'NumStorePurchases', 'NumWebVisitsMonth']].sum(axis=1))\n",
    "sum_group_Num.reshape(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db4e0aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumDealsPurchases</th>\n",
       "      <th>NumWebPurchases</th>\n",
       "      <th>NumCatalogPurchases</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.066961</td>\n",
       "      <td>0.224837</td>\n",
       "      <td>0.226893</td>\n",
       "      <td>0.349806</td>\n",
       "      <td>0.131503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144493</td>\n",
       "      <td>0.145501</td>\n",
       "      <td>0.035642</td>\n",
       "      <td>0.207898</td>\n",
       "      <td>0.466465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.142165</td>\n",
       "      <td>0.216571</td>\n",
       "      <td>0.102681</td>\n",
       "      <td>0.277008</td>\n",
       "      <td>0.261575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumDealsPurchases  NumWebPurchases  NumCatalogPurchases  NumStorePurchases  \\\n",
       "0           0.066961         0.224837             0.226893           0.349806   \n",
       "1           0.144493         0.145501             0.035642           0.207898   \n",
       "2           0.142165         0.216571             0.102681           0.277008   \n",
       "\n",
       "   NumWebVisitsMonth  \n",
       "0           0.131503  \n",
       "1           0.466465  \n",
       "2           0.261575  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cluster별 구매 채널의 비율\n",
    "group_mean[['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']] / sum_group_Num.reshape(3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59b77e",
   "metadata": {},
   "source": [
    "[해석]\n",
    "- 그룹 0: 매장에서 구매하는 비율이 가장 많음.\n",
    "- 그룹 1: 카탈로그를 사용하여 구매하는 비율이 가장 낮고, 지난 달 웹사이트를 방문한 비율이 타 그룹에 비교하면 가장 높음.\n",
    "- 그룹 2: 매장에서 구매하는 비율이 가장 높음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845c58b",
   "metadata": {},
   "source": [
    "#### (2) 각 군집별 상품을 추천하시오.\n",
    "[해석]\n",
    "- 그룹 0: 와인 추천\n",
    "- 그룹 1: 고기 추천\n",
    "- 그룹 2: 와인 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f99508a",
   "metadata": {},
   "source": [
    "#### (3) ID가 10870인 고객을 대상으로 상품을 추천하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cbec12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>NumDealsPurchases</th>\n",
       "      <th>NumWebPurchases</th>\n",
       "      <th>NumCatalogPurchases</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>10870</td>\n",
       "      <td>1967</td>\n",
       "      <td>Married</td>\n",
       "      <td>61223.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>709</td>\n",
       "      <td>43</td>\n",
       "      <td>182</td>\n",
       "      <td>42</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Year_Birth Marital_Status   Income  Kidhome  Teenhome  MntWines  \\\n",
       "2235  10870        1967        Married  61223.0        0         1       709   \n",
       "\n",
       "      MntFruits  MntMeatProducts  MntFishProducts  MntSweetProducts  \\\n",
       "2235         43              182               42               118   \n",
       "\n",
       "      NumDealsPurchases  NumWebPurchases  NumCatalogPurchases  \\\n",
       "2235                  2                9                    3   \n",
       "\n",
       "      NumStorePurchases  NumWebVisitsMonth  cluster  \n",
       "2235                  4                  5        2  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nonull[df_nonull['ID']==10870]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654679c",
   "metadata": {},
   "source": [
    "[해석]\n",
    "- 해당 고객은 그룹 2에 속하며 와인을 추천할 수 있음.\n",
    "- 그룹 2의 특성 상 웹사이트, 매장에서 구매하는 비율이 높았는데, 해당 고객은 지난달 구매수와 이번 달 구매수를 보니 웹사이트를 많이 이용하는 고객으로 보이므로 웹사이트를 추천하는 것이 좋음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e55a2b",
   "metadata": {},
   "source": [
    "## 통계분석\n",
    "### 1. 한 공장에서 생산된 제품에서 최근 추정 불량률은 90%였다. 오차의 한계가 5% 이하가 되도록 하는 최소 표본 사이즈를 구하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7243cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소 샘플 사이즈는 138.29759999999993보다 커야함.\n"
     ]
    }
   ],
   "source": [
    "def calculatae_sample_size(p, z, d):\n",
    "    n = p*(1-p)*(z/d)**2\n",
    "    print('최소 샘플 사이즈는 {}보다 커야함.'.format(n))\n",
    "calculatae_sample_size(0.9, 1.96, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a2dbc",
   "metadata": {},
   "source": [
    "### 2. 다음은 1월부터 9월까지의 은의 가격이다.\n",
    "#### (1) 은의 가격 및 이동편균값 3이 설정된 시계열 그래프를 그리시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21f25c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1M</th>\n",
       "      <th>2M</th>\n",
       "      <th>3M</th>\n",
       "      <th>4M</th>\n",
       "      <th>5M</th>\n",
       "      <th>6M</th>\n",
       "      <th>7M</th>\n",
       "      <th>8M</th>\n",
       "      <th>9M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.14</td>\n",
       "      <td>42.6</td>\n",
       "      <td>34.4</td>\n",
       "      <td>35.29</td>\n",
       "      <td>30.96</td>\n",
       "      <td>57.12</td>\n",
       "      <td>37.84</td>\n",
       "      <td>42.49</td>\n",
       "      <td>31.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1M    2M    3M     4M     5M     6M     7M     8M     9M\n",
       "0  12.14  42.6  34.4  35.29  30.96  57.12  37.84  42.49  31.38"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/26_problem4.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "946fbe9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1M</th>\n",
       "      <td>12.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2M</th>\n",
       "      <td>42.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3M</th>\n",
       "      <td>34.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4M</th>\n",
       "      <td>35.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5M</th>\n",
       "      <td>30.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6M</th>\n",
       "      <td>57.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7M</th>\n",
       "      <td>37.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8M</th>\n",
       "      <td>42.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9M</th>\n",
       "      <td>31.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month_price\n",
       "1M        12.14\n",
       "2M        42.60\n",
       "3M        34.40\n",
       "4M        35.29\n",
       "5M        30.96\n",
       "6M        57.12\n",
       "7M        37.84\n",
       "8M        42.49\n",
       "9M        31.38"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파생변수를 추가하기 위해서 행열변환\n",
    "ma_data = df.transpose()\n",
    "ma_data.columns = ['month_price']\n",
    "ma_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "982a8d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_price</th>\n",
       "      <th>ma_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1M</th>\n",
       "      <td>12.14</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2M</th>\n",
       "      <td>42.60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3M</th>\n",
       "      <td>34.40</td>\n",
       "      <td>29.713333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4M</th>\n",
       "      <td>35.29</td>\n",
       "      <td>37.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5M</th>\n",
       "      <td>30.96</td>\n",
       "      <td>33.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6M</th>\n",
       "      <td>57.12</td>\n",
       "      <td>41.123333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7M</th>\n",
       "      <td>37.84</td>\n",
       "      <td>41.973333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8M</th>\n",
       "      <td>42.49</td>\n",
       "      <td>45.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9M</th>\n",
       "      <td>31.38</td>\n",
       "      <td>37.236667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    month_price       ma_3\n",
       "1M        12.14   0.000000\n",
       "2M        42.60   0.000000\n",
       "3M        34.40  29.713333\n",
       "4M        35.29  37.430000\n",
       "5M        30.96  33.550000\n",
       "6M        57.12  41.123333\n",
       "7M        37.84  41.973333\n",
       "8M        42.49  45.816667\n",
       "9M        31.38  37.236667"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파생변수 추가\n",
    "## 3개월 이동평균 값을 ma_3에 저장함\n",
    "ma_data['ma_3']=0\n",
    "ma_data.loc['3M', 'ma_3'] = ma_data['month_price'][0:3].mean()\n",
    "ma_data.loc['4M', 'ma_3'] = ma_data['month_price'][1:4].mean()\n",
    "ma_data.loc['5M', 'ma_3'] = ma_data['month_price'][2:5].mean()\n",
    "ma_data.loc['6M', 'ma_3'] = ma_data['month_price'][3:6].mean()\n",
    "ma_data.loc['7M', 'ma_3'] = ma_data['month_price'][4:7].mean()\n",
    "ma_data.loc['8M', 'ma_3'] = ma_data['month_price'][5:8].mean()\n",
    "ma_data.loc['9M', 'ma_3'] = ma_data['month_price'][6:9].mean()\n",
    "ma_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "197de8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 시각화\n",
    "plt.scatter(x=ma_data.index, y='month_price', data=ma_data)\n",
    "plt.plot(ma_data.index[2:], ma_data['ma_3'][2:], c='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da898ed0",
   "metadata": {},
   "source": [
    "#### (2) 1월 대비 9월의 은의 가격은 몇 % 올랐는가?(소수점 두 번째 자리에서 반올림)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "811b07f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258.5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(ma_data['month_price'][-1] / ma_data['month_price'][0] *100, 1) # 9월 가격 / 1월 가격 * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a076b",
   "metadata": {},
   "source": [
    "### 3. 아래 그래프는 A, B, C 자치구별 H의원에 대한 찬성, 반대 투표 결과이다. 자치구별 지지율이 같은지에 대해서 검정하시오.\n",
    "[가설]\n",
    "- 귀무가설: 자치구와 지지율은 서로 독립이다.\n",
    "- 대립가설: 자치구와 지지율은 서로 독립이 아니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18bd910e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>찬성</th>\n",
       "      <td>176</td>\n",
       "      <td>193</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>반대</th>\n",
       "      <td>124</td>\n",
       "      <td>107</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A    B    C\n",
       "찬성  176  193  159\n",
       "반대  124  107  141"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({\n",
    "    'A': [176, 124],\n",
    "    'B': [193, 107],\n",
    "    'C': [159, 141]\n",
    "}, index=['찬성', '반대'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3c399b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistic: 7.945381231671554\n",
      "p-value: 0.01882272023214683\n",
      "df: 2\n",
      "expect:\n",
      " [[176. 176. 176.]\n",
      " [124. 124. 124.]]\n"
     ]
    }
   ],
   "source": [
    "# 독립성 검정\n",
    "from scipy.stats import chi2_contingency\n",
    "chi, p, df, expect = chi2_contingency(data)\n",
    "print('Statistic:', chi)\n",
    "print('p-value:', p)\n",
    "print('df:', df)\n",
    "print('expect:\\n', expect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3465a16",
   "metadata": {},
   "source": [
    "### 4. A학교 남녀 학생들의 평균 혈압 차이가 있는지 여부에 대한 검정하시오.(단, 남학생과 여학생의 혈압 데이터는 정규분포를 따르며 등분산임을 가정)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a553bb21",
   "metadata": {},
   "source": [
    "#### (1) 남녀 학생들의 평균 혈압 차이가 있는지에 대해 가설을 설정하시오.\n",
    "[가설]\n",
    "- 귀무가설: 성별에 따른 평균 혈압에 차이가 없다.\n",
    "- 대립가설: 성별에 따른 평균 혈압에 차이가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b3498",
   "metadata": {},
   "source": [
    "#### (2) 검정통계량을 구하고 판단하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27870367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>106.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>100.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>84.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>104.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  pressure\n",
       "0   male     106.8\n",
       "1   male     100.8\n",
       "2   male      84.5\n",
       "3   male     104.2\n",
       "4   male     107.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/26_problem6.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d84b316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.3813481801194591, pvalue=0.18044550626193734)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "male = df.loc[df.gender=='male', 'pressure']\n",
    "female = df.loc[df.gender=='female', 'pressure']\n",
    "\n",
    "stats.ttest_ind(male, female, equal_var=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1345a598",
   "metadata": {},
   "source": [
    "[해석]\n",
    "- p-value > 0.05 이므로 귀무가설을 기각하지 못하므로, 성별에 따른 평균 혈압에 차이가 없다고 할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b13dc",
   "metadata": {},
   "source": [
    "#### (3) 평균 혈압차의 신뢰구간 구했을 때, 판단한 결과가 (2)의 결과를 지지하는지 설명하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ff1b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, mean\n",
    "from scipy.stats import sem, t\n",
    "import numpy as np\n",
    "\n",
    "# 통합 표준편차(sp)를 구하는 공식이 복합하므로 함수를 만들어 사용\n",
    "def sp(data1, data2) : \n",
    "    df = len(data) + len(data2) -2\n",
    "    s1 = (len(data1)-1) * (data1.std()**2)\n",
    "    s2 = (len(data2)-1) * (data2.std()**2)\n",
    "    sp = np.sqrt((s1+s2)/ df)\n",
    "    return sp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c08d09fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7210256360769041, 9.872029919478662)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.95 \n",
    "dof = len(male) + len(female) -2\n",
    "diff_mean = np.mean(male) - np.mean(female)\n",
    "s =  sp(male,female) * (1/len(male) + 1/len(female))\n",
    "\n",
    "CI = stats.t.interval(alpha, dof, loc = diff_mean, scale = s) \n",
    "CI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32459e47",
   "metadata": {},
   "source": [
    "### 5. height(키), weight(몸무게), waist(허리둘레) 컬럼을 가진 problem7.csv파일을 가지고 다음을 분석하시오 A시의 20대 남성 411명을 임의로 추출하여 키, 몸무게, 허리둘레를 조사하여 기록한 데이터이다. 이 데이터를 이용하여 20대 남성의 키와 허리둘레가 체중에 영향을 미치는지 알아보시오.\n",
    "#### (1) 아래 조건을 참고하여 회귀계수(반올림하여 소수점 두 자리)를 구하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61fead42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>waistline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174.396</td>\n",
       "      <td>72.102</td>\n",
       "      <td>79.3787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179.656</td>\n",
       "      <td>81.255</td>\n",
       "      <td>80.6649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175.079</td>\n",
       "      <td>76.207</td>\n",
       "      <td>80.3166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180.804</td>\n",
       "      <td>81.354</td>\n",
       "      <td>80.8794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>177.448</td>\n",
       "      <td>78.768</td>\n",
       "      <td>80.3499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    height  weight  waistline\n",
       "0  174.396  72.102    79.3787\n",
       "1  179.656  81.255    80.6649\n",
       "2  175.079  76.207    80.3166\n",
       "3  180.804  81.354    80.8794\n",
       "4  177.448  78.768    80.3499"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/26_problem7.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb17ac6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.linear_model in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.linear_model - The :mod:`sklearn.linear_model` module implements a variety of linear models.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _bayes\n",
      "    _cd_fast\n",
      "    _coordinate_descent\n",
      "    _glm (package)\n",
      "    _huber\n",
      "    _least_angle\n",
      "    _logistic\n",
      "    _omp\n",
      "    _passive_aggressive\n",
      "    _perceptron\n",
      "    _ransac\n",
      "    _ridge\n",
      "    _sag\n",
      "    _sag_fast\n",
      "    _sgd_fast\n",
      "    _stochastic_gradient\n",
      "    _theil_sen\n",
      "    base\n",
      "    bayes\n",
      "    cd_fast\n",
      "    coordinate_descent\n",
      "    huber\n",
      "    least_angle\n",
      "    logistic\n",
      "    omp\n",
      "    passive_aggressive\n",
      "    perceptron\n",
      "    ransac\n",
      "    ridge\n",
      "    sag\n",
      "    sag_fast\n",
      "    setup\n",
      "    sgd_fast\n",
      "    stochastic_gradient\n",
      "    tests (package)\n",
      "    theil_sen\n",
      "    ~ests (package)\n",
      "    ~glm (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.linear_model._huber.HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._logistic.LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin)\n",
      "            sklearn.linear_model._logistic.LogisticRegressionCV(sklearn.linear_model._logistic.LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model._base.LinearClassifierMixin)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.MetaEstimatorMixin(builtins.object)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.MultiOutputMixin(builtins.object)\n",
      "        sklearn.linear_model._base.LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._coordinate_descent.Lasso\n",
      "                sklearn.linear_model._coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model._coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model._least_angle.Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._least_angle.LarsCV\n",
      "                sklearn.linear_model._least_angle.LassoLarsCV\n",
      "            sklearn.linear_model._least_angle.LassoLars\n",
      "                sklearn.linear_model._least_angle.LassoLarsIC\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ridge.Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.linear_model._base.LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.ARDRegression(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.BayesianRidge(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._coordinate_descent.Lasso\n",
      "                sklearn.linear_model._coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model._coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.LassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskLassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._huber.HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._least_angle.Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._least_angle.LarsCV\n",
      "                sklearn.linear_model._least_angle.LassoLarsCV\n",
      "            sklearn.linear_model._least_angle.LassoLars\n",
      "                sklearn.linear_model._least_angle.LassoLarsIC\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuitCV(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ridge.Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "        sklearn.linear_model._theil_sen.TheilSenRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "    sklearn.linear_model._base.LinearClassifierMixin(sklearn.base.ClassifierMixin)\n",
      "        sklearn.linear_model._logistic.LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin)\n",
      "            sklearn.linear_model._logistic.LogisticRegressionCV(sklearn.linear_model._logistic.LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model._base.LinearClassifierMixin)\n",
      "        sklearn.linear_model._ridge.RidgeClassifier(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeClassifierCV(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "    sklearn.linear_model._base.LinearModel(sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._base.LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.ARDRegression(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.BayesianRidge(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._coordinate_descent.Lasso\n",
      "                sklearn.linear_model._coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model._coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model._huber.HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._least_angle.Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._least_angle.LarsCV\n",
      "                sklearn.linear_model._least_angle.LassoLarsCV\n",
      "            sklearn.linear_model._least_angle.LassoLars\n",
      "                sklearn.linear_model._least_angle.LassoLarsIC\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuitCV(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._theil_sen.TheilSenRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "    sklearn.linear_model._base.SparseCoefMixin(builtins.object)\n",
      "        sklearn.linear_model._logistic.LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin)\n",
      "            sklearn.linear_model._logistic.LogisticRegressionCV(sklearn.linear_model._logistic.LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model._base.LinearClassifierMixin)\n",
      "    sklearn.linear_model._coordinate_descent.LinearModelCV(sklearn.base.MultiOutputMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.LassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskLassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "    sklearn.linear_model._glm.glm.GeneralizedLinearRegressor(sklearn.base.BaseEstimator, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model._glm.glm.GammaRegressor\n",
      "        sklearn.linear_model._glm.glm.PoissonRegressor\n",
      "        sklearn.linear_model._glm.glm.TweedieRegressor\n",
      "    sklearn.linear_model._ridge._BaseRidge(sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ridge.Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeClassifier(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "    sklearn.linear_model._ridge._BaseRidgeCV(sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ridge.RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "        sklearn.linear_model._ridge.RidgeClassifierCV(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "    sklearn.linear_model._sgd_fast.Classification(sklearn.linear_model._sgd_fast.LossFunction)\n",
      "        sklearn.linear_model._sgd_fast.Hinge\n",
      "        sklearn.linear_model._sgd_fast.Log\n",
      "        sklearn.linear_model._sgd_fast.ModifiedHuber\n",
      "    sklearn.linear_model._sgd_fast.Regression(sklearn.linear_model._sgd_fast.LossFunction)\n",
      "        sklearn.linear_model._sgd_fast.Huber\n",
      "        sklearn.linear_model._sgd_fast.SquaredLoss\n",
      "    sklearn.linear_model._stochastic_gradient.BaseSGDClassifier(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._stochastic_gradient.BaseSGD)\n",
      "        sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier\n",
      "        sklearn.linear_model._perceptron.Perceptron\n",
      "        sklearn.linear_model._stochastic_gradient.SGDClassifier\n",
      "    sklearn.linear_model._stochastic_gradient.BaseSGDRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._stochastic_gradient.BaseSGD)\n",
      "        sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor\n",
      "        sklearn.linear_model._stochastic_gradient.SGDRegressor\n",
      "    \n",
      "    class ARDRegression(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  ARDRegression(*, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      "     |  \n",
      "     |  Bayesian ARD regression.\n",
      "     |  \n",
      "     |  Fit the weights of a regression model, using an ARD prior. The weights of\n",
      "     |  the regression model are assumed to be in Gaussian distributions.\n",
      "     |  Also estimate the parameters lambda (precisions of the distributions of the\n",
      "     |  weights) and alpha (precision of the distribution of the noise).\n",
      "     |  The estimation is done by an iterative procedures (Evidence Maximization)\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_iter : int, default=300\n",
      "     |      Maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Stop the algorithm if w has converged.\n",
      "     |  \n",
      "     |  alpha_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the alpha parameter.\n",
      "     |  \n",
      "     |  alpha_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the alpha parameter.\n",
      "     |  \n",
      "     |  lambda_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the lambda parameter.\n",
      "     |  \n",
      "     |  lambda_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the lambda parameter.\n",
      "     |  \n",
      "     |  compute_score : bool, default=False\n",
      "     |      If True, compute the objective function at each step of the model.\n",
      "     |  \n",
      "     |  threshold_lambda : float, default=10 000\n",
      "     |      threshold for removing (pruning) weights with high precision from\n",
      "     |      the computation.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      Coefficients of the regression model (mean of distribution)\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |     estimated precision of the noise.\n",
      "     |  \n",
      "     |  lambda_ : array-like of shape (n_features,)\n",
      "     |     estimated precisions of the weights.\n",
      "     |  \n",
      "     |  sigma_ : array-like of shape (n_features, n_features)\n",
      "     |      estimated variance-covariance matrix of the weights\n",
      "     |  \n",
      "     |  scores_ : float\n",
      "     |      if computed, value of the objective function (to be maximized)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.ARDRegression()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  ARDRegression()\n",
      "     |  >>> clf.predict([[1, 1]])\n",
      "     |  array([1.])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see :ref:`examples/linear_model/plot_ard.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\n",
      "     |  competition, ASHRAE Transactions, 1994.\n",
      "     |  \n",
      "     |  R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n",
      "     |  http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n",
      "     |  Their beta is our ``self.alpha_``\n",
      "     |  Their alpha is our ``self.lambda_``\n",
      "     |  ARD is a little different than the slide: only dimensions/features for\n",
      "     |  which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\n",
      "     |  discarded.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ARDRegression\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the ARDRegression model according to the given training data\n",
      "     |      and parameters.\n",
      "     |      \n",
      "     |      Iterative procedure to maximize the evidence\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (integers). Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X, return_std=False)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      In addition to the mean of the predictive distribution, also its\n",
      "     |      standard deviation can be returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      return_std : bool, default=False\n",
      "     |          Whether to return the standard deviation of posterior prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_mean : array-like of shape (n_samples,)\n",
      "     |          Mean of predictive distribution of query points.\n",
      "     |      \n",
      "     |      y_std : array-like of shape (n_samples,)\n",
      "     |          Standard deviation of predictive distribution of query points.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class BayesianRidge(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  BayesianRidge(*, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      "     |  \n",
      "     |  Bayesian ridge regression.\n",
      "     |  \n",
      "     |  Fit a Bayesian ridge model. See the Notes section for details on this\n",
      "     |  implementation and the optimization of the regularization parameters\n",
      "     |  lambda (precision of the weights) and alpha (precision of the noise).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_iter : int, default=300\n",
      "     |      Maximum number of iterations. Should be greater than or equal to 1.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Stop the algorithm if w has converged.\n",
      "     |  \n",
      "     |  alpha_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the alpha parameter.\n",
      "     |  \n",
      "     |  alpha_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the alpha parameter.\n",
      "     |  \n",
      "     |  lambda_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the lambda parameter.\n",
      "     |  \n",
      "     |  lambda_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the lambda parameter.\n",
      "     |  \n",
      "     |  alpha_init : float, default=None\n",
      "     |      Initial value for alpha (precision of the noise).\n",
      "     |      If not set, alpha_init is 1/Var(y).\n",
      "     |  \n",
      "     |          .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  lambda_init : float, default=None\n",
      "     |      Initial value for lambda (precision of the weights).\n",
      "     |      If not set, lambda_init is 1.\n",
      "     |  \n",
      "     |          .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  compute_score : bool, default=False\n",
      "     |      If True, compute the log marginal likelihood at each iteration of the\n",
      "     |      optimization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model.\n",
      "     |      The intercept is not treated as a probabilistic parameter\n",
      "     |      and thus has no associated variance. If set\n",
      "     |      to False, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      Coefficients of the regression model (mean of distribution)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |     Estimated precision of the noise.\n",
      "     |  \n",
      "     |  lambda_ : float\n",
      "     |     Estimated precision of the weights.\n",
      "     |  \n",
      "     |  sigma_ : array-like of shape (n_features, n_features)\n",
      "     |      Estimated variance-covariance matrix of the weights\n",
      "     |  \n",
      "     |  scores_ : array-like of shape (n_iter_+1,)\n",
      "     |      If computed_score is True, value of the log marginal likelihood (to be\n",
      "     |      maximized) at each iteration of the optimization. The array starts\n",
      "     |      with the value of the log marginal likelihood obtained for the initial\n",
      "     |      values of alpha and lambda and ends with the value obtained for the\n",
      "     |      estimated alpha and lambda.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.BayesianRidge()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  BayesianRidge()\n",
      "     |  >>> clf.predict([[1, 1]])\n",
      "     |  array([1.])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  There exist several strategies to perform Bayesian ridge regression. This\n",
      "     |  implementation is based on the algorithm described in Appendix A of\n",
      "     |  (Tipping, 2001) where updates of the regularization parameters are done as\n",
      "     |  suggested in (MacKay, 1992). Note that according to A New\n",
      "     |  View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\n",
      "     |  update rules do not guarantee that the marginal likelihood is increasing\n",
      "     |  between two consecutive iterations of the optimization.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n",
      "     |  Vol. 4, No. 3, 1992.\n",
      "     |  \n",
      "     |  M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\n",
      "     |  Journal of Machine Learning Research, Vol. 1, 2001.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BayesianRidge\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      sample_weight : ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample\n",
      "     |      \n",
      "     |          .. versionadded:: 0.20\n",
      "     |             parameter *sample_weight* support to BayesianRidge.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X, return_std=False)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      In addition to the mean of the predictive distribution, also its\n",
      "     |      standard deviation can be returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      return_std : bool, default=False\n",
      "     |          Whether to return the standard deviation of posterior prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_mean : array-like of shape (n_samples,)\n",
      "     |          Mean of predictive distribution of query points.\n",
      "     |      \n",
      "     |      y_std : array-like of shape (n_samples,)\n",
      "     |          Standard deviation of predictive distribution of query points.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  ElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |  \n",
      "     |  Minimizes the objective function::\n",
      "     |  \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |  \n",
      "     |  If you are interested in controlling the L1 and L2 penalty\n",
      "     |  separately, keep in mind that this is equivalent to::\n",
      "     |  \n",
      "     |          a * L1 + b * L2\n",
      "     |  \n",
      "     |  where::\n",
      "     |  \n",
      "     |          alpha = a + b and l1_ratio = a / (a + b)\n",
      "     |  \n",
      "     |  The parameter l1_ratio corresponds to alpha in the glmnet R package while\n",
      "     |  alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n",
      "     |  = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n",
      "     |  unless you supply your own sequence of alpha.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the penalty terms. Defaults to 1.0.\n",
      "     |      See the notes for the exact mathematical meaning of this\n",
      "     |      parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n",
      "     |      solved by the :class:`LinearRegression` object. For numerical\n",
      "     |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      "     |      Given this, you should use the :class:`LinearRegression` object.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.5\n",
      "     |      The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n",
      "     |      ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n",
      "     |      is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n",
      "     |      combination of L1 and L2.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If ``False``, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. The Gram matrix can also be passed as argument.\n",
      "     |      For sparse input this option is always ``True`` to preserve sparsity.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      parameter vector (w in the cost function formula)\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n",
      "     |      ``sparse_coef_`` is a readonly property derived from ``coef_``\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : list of int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import ElasticNet\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  \n",
      "     |  >>> X, y = make_regression(n_features=2, random_state=0)\n",
      "     |  >>> regr = ElasticNet(random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  ElasticNet(random_state=0)\n",
      "     |  >>> print(regr.coef_)\n",
      "     |  [18.83816048 64.55968825]\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  1.451...\n",
      "     |  >>> print(regr.predict([[0, 0]]))\n",
      "     |  [1.451...]\n",
      "     |  \n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  ElasticNetCV : Elastic net model with best model selection by\n",
      "     |      cross-validation.\n",
      "     |  SGDRegressor: implements elastic net regression with incremental training.\n",
      "     |  SGDClassifier: implements logistic regression with elastic net penalty\n",
      "     |      (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True)\n",
      "     |      Fit model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of (n_samples, n_features)\n",
      "     |          Data\n",
      "     |      \n",
      "     |      y : {ndarray, sparse matrix} of shape (n_samples,) or             (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape (n_samples,), default=None\n",
      "     |          Sample weight.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      sparse representation of the fitted ``coef_``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ElasticNetCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  ElasticNetCV(*, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Elastic Net model with iterative fitting along a regularization path.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  l1_ratio : float or list of float, default=0.5\n",
      "     |      float between 0 and 1 passed to ElasticNet (scaling between\n",
      "     |      l1 and l2 penalties). For ``l1_ratio = 0``\n",
      "     |      the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\n",
      "     |      This parameter can be a list, in which case the different\n",
      "     |      values are tested by cross-validation and the one giving the best\n",
      "     |      prediction score is used. Note that a good choice of list of\n",
      "     |      values for l1_ratio is often to put more values close to 1\n",
      "     |      (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n",
      "     |      .9, .95, .99, 1]``\n",
      "     |  \n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path, used for each l1_ratio.\n",
      "     |  \n",
      "     |  alphas : ndarray, default=None\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If None alphas are set automatically\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=0\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation\n",
      "     |  \n",
      "     |  l1_ratio_ : float\n",
      "     |      The compromise between l1 and l2 penalization chosen by\n",
      "     |      cross validation\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula),\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets, n_features)\n",
      "     |      Independent term in the decision function.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying l1_ratio and\n",
      "     |      alpha.\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n",
      "     |      The grid of alphas used for fitting, for each l1_ratio.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import ElasticNetCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  \n",
      "     |  >>> X, y = make_regression(n_features=2, random_state=0)\n",
      "     |  >>> regr = ElasticNetCV(cv=5, random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  ElasticNetCV(cv=5, random_state=0)\n",
      "     |  >>> print(regr.alpha_)\n",
      "     |  0.199...\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  0.398...\n",
      "     |  >>> print(regr.predict([[0, 0]]))\n",
      "     |  [0.398...]\n",
      "     |  \n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see\n",
      "     |  :ref:`examples/linear_model/plot_lasso_model_selection.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  The parameter l1_ratio corresponds to alpha in the glmnet R package\n",
      "     |  while alpha corresponds to the lambda parameter in glmnet.\n",
      "     |  More specifically, the optimization objective is::\n",
      "     |  \n",
      "     |      1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |      + alpha * l1_ratio * ||w||_1\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |  \n",
      "     |  If you are interested in controlling the L1 and L2 penalty\n",
      "     |  separately, keep in mind that this is equivalent to::\n",
      "     |  \n",
      "     |      a * L1 + b * L2\n",
      "     |  \n",
      "     |  for::\n",
      "     |  \n",
      "     |      alpha = a + b and l1_ratio = a / (a + b).\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  enet_path\n",
      "     |  ElasticNet\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ElasticNetCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class GammaRegressor(GeneralizedLinearRegressor)\n",
      "     |  GammaRegressor(*, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |  \n",
      "     |  Generalized Linear Model with a Gamma distribution.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <Generalized_linear_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1\n",
      "     |      Constant that multiplies the penalty term and thus determines the\n",
      "     |      regularization strength. ``alpha = 0`` is equivalent to unpenalized\n",
      "     |      GLMs. In this case, the design matrix `X` must have full column rank\n",
      "     |      (no collinearities).\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the linear predictor (X @ coef + intercept).\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximal number of iterations for the solver.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Stopping criterion. For the lbfgs solver,\n",
      "     |      the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n",
      "     |      where ``g_j`` is the j-th component of the gradient (derivative) of\n",
      "     |      the objective function.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If set to ``True``, reuse the solution of the previous call to ``fit``\n",
      "     |      as initialization for ``coef_`` and ``intercept_`` .\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the lbfgs solver set verbose to any positive number for verbosity.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the linear predictor (`X * coef_ +\n",
      "     |      intercept_`) in the GLM.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Intercept (a.k.a. bias) added to linear predictor.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Actual number of iterations used in the solver.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GammaRegressor\n",
      "     |      GeneralizedLinearRegressor\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  family\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from GeneralizedLinearRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit a Generalized Linear Model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using GLM with feature matrix X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array of shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Compute D^2, the percentage of deviance explained.\n",
      "     |      \n",
      "     |      D^2 is a generalization of the coefficient of determination R^2.\n",
      "     |      R^2 uses squared error and D^2 deviance. Note that those two are equal\n",
      "     |      for ``family='normal'``.\n",
      "     |      \n",
      "     |      D^2 is defined as\n",
      "     |      :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n",
      "     |      :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n",
      "     |      with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n",
      "     |      The mean :math:`\\bar{y}` is averaged by sample_weight.\n",
      "     |      Best possible score is 1.0 and it can be negative (because the model\n",
      "     |      can be arbitrarily worse).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True values of target.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          D^2 of self.predict(X) w.r.t. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Hinge(Classification)\n",
      "     |  Hinge loss for binary classification tasks with y in {-1,1}\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  threshold : float > 0.0\n",
      "     |      Margin threshold. When threshold=1.0, one gets the loss used by SVM.\n",
      "     |      When threshold=0.0, one gets the loss used by the Perceptron.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Hinge\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Classification:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class Huber(Regression)\n",
      "     |  Huber regression loss\n",
      "     |  \n",
      "     |  Variant of the SquaredLoss that is robust to outliers (quadratic near zero,\n",
      "     |  linear in for large errors).\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/Huber_Loss_Function\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Huber\n",
      "     |      Regression\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Regression:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "     |  HuberRegressor(*, epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)\n",
      "     |  \n",
      "     |  Linear regression model that is robust to outliers.\n",
      "     |  \n",
      "     |  The Huber Regressor optimizes the squared loss for the samples where\n",
      "     |  ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples\n",
      "     |  where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters\n",
      "     |  to be optimized. The parameter sigma makes sure that if y is scaled up\n",
      "     |  or down by a certain factor, one does not need to rescale epsilon to\n",
      "     |  achieve the same robustness. Note that this does not take into account\n",
      "     |  the fact that the different features of X may be of different scales.\n",
      "     |  \n",
      "     |  This makes sure that the loss function is not heavily influenced by the\n",
      "     |  outliers while not completely ignoring their effect.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <huber_regression>`\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  epsilon : float, greater than 1.0, default 1.35\n",
      "     |      The parameter epsilon controls the number of samples that should be\n",
      "     |      classified as outliers. The smaller the epsilon, the more robust it is\n",
      "     |      to outliers.\n",
      "     |  \n",
      "     |  max_iter : int, default 100\n",
      "     |      Maximum number of iterations that\n",
      "     |      ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` should run for.\n",
      "     |  \n",
      "     |  alpha : float, default 0.0001\n",
      "     |      Regularization parameter.\n",
      "     |  \n",
      "     |  warm_start : bool, default False\n",
      "     |      This is useful if the stored attributes of a previously used model\n",
      "     |      has to be reused. If set to False, then the coefficients will\n",
      "     |      be rewritten for every call to fit.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default True\n",
      "     |      Whether or not to fit the intercept. This can be set to False\n",
      "     |      if the data is already centered around the origin.\n",
      "     |  \n",
      "     |  tol : float, default 1e-5\n",
      "     |      The iteration will stop when\n",
      "     |      ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\n",
      "     |      where pg_i is the i-th component of the projected gradient.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      Features got by optimizing the Huber loss.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Bias.\n",
      "     |  \n",
      "     |  scale_ : float\n",
      "     |      The value by which ``|y - X'w - c|`` is scaled down.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations that\n",
      "     |      ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` has run for.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |  \n",
      "     |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      "     |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      "     |  \n",
      "     |  outliers_ : array, shape (n_samples,)\n",
      "     |      A boolean mask which is set to True where the samples are identified\n",
      "     |      as outliers.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import HuberRegressor, LinearRegression\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> X, y, coef = make_regression(\n",
      "     |  ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\n",
      "     |  >>> X[:4] = rng.uniform(10, 20, (4, 2))\n",
      "     |  >>> y[:4] = rng.uniform(10, 20, 4)\n",
      "     |  >>> huber = HuberRegressor().fit(X, y)\n",
      "     |  >>> huber.score(X, y)\n",
      "     |  -7.284...\n",
      "     |  >>> huber.predict(X[:1,])\n",
      "     |  array([806.7200...])\n",
      "     |  >>> linear = LinearRegression().fit(X, y)\n",
      "     |  >>> print(\"True coefficients:\", coef)\n",
      "     |  True coefficients: [20.4923...  34.1698...]\n",
      "     |  >>> print(\"Huber coefficients:\", huber.coef_)\n",
      "     |  Huber coefficients: [17.7906... 31.0106...]\n",
      "     |  >>> print(\"Linear Regression coefficients:\", linear.coef_)\n",
      "     |  Linear Regression coefficients: [-1.9221...  7.0226...]\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\n",
      "     |         Concomitant scale estimates, pg 172\n",
      "     |  .. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.\n",
      "     |         https://statweb.stanford.edu/~owen/reports/hhu.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HuberRegressor\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Weight given to each sample.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  Lars(*, fit_intercept=True, verbose=False, normalize=True, precompute='auto', n_nonzero_coefs=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, jitter=None, random_state=None)\n",
      "     |  \n",
      "     |  Least Angle Regression model a.k.a. LAR\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like , default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  n_nonzero_coefs : int, default=500\n",
      "     |      Target number of non-zero coefficients. Use ``np.inf`` for no limit.\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |      By default, ``np.finfo(np.float).eps`` is used.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  fit_path : bool, default=True\n",
      "     |      If True the full path is stored in the ``coef_path_`` attribute.\n",
      "     |      If you compute the solution for a large problem or many targets,\n",
      "     |      setting ``fit_path`` to ``False`` will lead to a speedup, especially\n",
      "     |      with a small alpha.\n",
      "     |  \n",
      "     |  jitter : float, default=None\n",
      "     |      Upper bound on a uniform noise parameter to be added to the\n",
      "     |      `y` values, to satisfy the model's assumption of\n",
      "     |      one-at-a-time computations. Might help with stability.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None (default)\n",
      "     |      Determines random number generation for jittering. Pass an int\n",
      "     |      for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alphas_ : array-like of shape (n_alphas + 1,) | list of n_targets such             arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``n_nonzero_coefs`` or ``n_features``,         whichever is smaller.\n",
      "     |  \n",
      "     |  active_ : list, length = n_alphas | list of n_targets such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas + 1)         | list of n_targets such arrays\n",
      "     |      The varying values of the coefficients along the path. It is not\n",
      "     |      present if the ``fit_path`` parameter is ``False``.\n",
      "     |  \n",
      "     |  coef_ : array-like of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formulation formula).\n",
      "     |  \n",
      "     |  intercept_ : float or array-like of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      The number of iterations taken by lars_path to find the\n",
      "     |      grid of alphas for each target.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.Lars(n_nonzero_coefs=1)\n",
      "     |  >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n",
      "     |  Lars(n_nonzero_coefs=1)\n",
      "     |  >>> print(reg.coef_)\n",
      "     |  [ 0. -1.11...]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path, LarsCV\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, verbose=False, normalize=True, precompute='auto', n_nonzero_coefs=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, jitter=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, Xy=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lar'\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LarsCV(Lars)\n",
      "     |  LarsCV(*, fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True)\n",
      "     |  \n",
      "     |  Cross-validated Least Angle Regression model.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like , default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram matrix\n",
      "     |      cannot be passed as argument since we will use only subsets of X.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  max_n_alphas : int, default=1000\n",
      "     |      The maximum number of points on the path used to compute the\n",
      "     |      residuals in the cross-validation\n",
      "     |  \n",
      "     |  n_jobs : int or None, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. By default, ``np.finfo(np.float).eps`` is used.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas)\n",
      "     |      the varying values of the coefficients along the path\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the estimated regularization parameter alpha\n",
      "     |  \n",
      "     |  alphas_ : array-like of shape (n_alphas,)\n",
      "     |      the different values of alpha along the path\n",
      "     |  \n",
      "     |  cv_alphas_ : array-like of shape (n_cv_alphas,)\n",
      "     |      all the values of alpha along the path for the different folds\n",
      "     |  \n",
      "     |  mse_path_ : array-like of shape (n_folds, n_cv_alphas)\n",
      "     |      the mean square error on left-out for each fold along the path\n",
      "     |      (alpha values given by ``cv_alphas``)\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      the number of iterations run by Lars with the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import LarsCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\n",
      "     |  >>> reg = LarsCV(cv=5).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9996...\n",
      "     |  >>> reg.alpha_\n",
      "     |  0.0254...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([154.0842...])\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path, LassoLars, LassoLarsCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LarsCV\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lar'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Lasso(ElasticNet)\n",
      "     |  Lasso(alpha=1.0, *, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Linear Model trained with L1 prior as regularizer (aka the Lasso)\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Technically the Lasso model is optimizing the same objective function as\n",
      "     |  the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the L1 term. Defaults to 1.0.\n",
      "     |      ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      "     |      by the :class:`LinearRegression` object. For numerical\n",
      "     |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      "     |      Given this, you should use the :class:`LinearRegression` object.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to False, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default=False\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument. For sparse input\n",
      "     |      this option is always ``True`` to preserve sparsity.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      parameter vector (w in the cost function formula)\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n",
      "     |      ``sparse_coef_`` is a readonly property derived from ``coef_``\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int or list of int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.Lasso(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  Lasso(alpha=0.1)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [0.85 0.  ]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  0.15...\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path\n",
      "     |  lasso_path\n",
      "     |  LassoLars\n",
      "     |  LassoCV\n",
      "     |  LassoLarsCV\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ElasticNet:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True)\n",
      "     |      Fit model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of (n_samples, n_features)\n",
      "     |          Data\n",
      "     |      \n",
      "     |      y : {ndarray, sparse matrix} of shape (n_samples,) or             (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape (n_samples,), default=None\n",
      "     |          Sample weight.\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      sparse representation of the fitted ``coef_``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LassoCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  LassoCV(*, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Lasso linear model with iterative fitting along a regularization path.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The best model is selected by cross-validation.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path\n",
      "     |  \n",
      "     |  alphas : ndarray, default=None\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If ``None`` alphas are set automatically\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      If positive, restrict regression coefficients to be positive\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      parameter vector (w in the cost function formula)\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_alphas, n_folds)\n",
      "     |      mean square error for the test set on each fold, varying alpha\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,)\n",
      "     |      The grid of alphas used for fitting\n",
      "     |  \n",
      "     |  dual_gap_ : float or ndarray of shape (n_targets,)\n",
      "     |      The dual gap at the end of the optimization for the optimal alpha\n",
      "     |      (``alpha_``).\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import LassoCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(noise=4, random_state=0)\n",
      "     |  >>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9993...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.4951...])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see\n",
      "     |  :ref:`examples/linear_model/plot_lasso_model_selection.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path\n",
      "     |  lasso_path\n",
      "     |  LassoLars\n",
      "     |  Lasso\n",
      "     |  LassoLarsCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "     |      Compute Lasso path with coordinate descent\n",
      "     |      \n",
      "     |      The Lasso optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <lasso>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If ``None`` alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |      should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |      \n",
      "     |      Note that in certain cases, the Lars solver may be significantly\n",
      "     |      faster to implement this functionality. In particular, linear\n",
      "     |      interpolation can be used to retrieve model coefficients between the\n",
      "     |      values output by lars_path\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      Comparing lasso_path and lars_path with interpolation:\n",
      "     |      \n",
      "     |      >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "     |      >>> y = np.array([1, 2, 3.1])\n",
      "     |      >>> # Use lasso_path to compute a coefficient path\n",
      "     |      >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "     |      >>> print(coef_path)\n",
      "     |      [[0.         0.         0.46874778]\n",
      "     |       [0.2159048  0.4425765  0.23689075]]\n",
      "     |      \n",
      "     |      >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "     |      >>> # same path\n",
      "     |      >>> from sklearn.linear_model import lars_path\n",
      "     |      >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "     |      >>> from scipy import interpolate\n",
      "     |      >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "     |      ...                                             coef_path_lars[:, ::-1])\n",
      "     |      >>> print(coef_path_continuous([5., 1., .5]))\n",
      "     |      [[0.         0.         0.46915237]\n",
      "     |       [0.2159048  0.4425765  0.23668876]]\n",
      "     |      \n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      lars_path\n",
      "     |      Lasso\n",
      "     |      LassoLars\n",
      "     |      LassoCV\n",
      "     |      LassoLarsCV\n",
      "     |      sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LassoLars(Lars)\n",
      "     |  LassoLars(alpha=1.0, *, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, positive=False, jitter=None, random_state=None)\n",
      "     |  \n",
      "     |  Lasso model fit with Least Angle Regression a.k.a. Lars\n",
      "     |  \n",
      "     |  It is a Linear Model trained with an L1 prior as regularizer.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the penalty term. Defaults to 1.0.\n",
      "     |      ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      "     |      by :class:`LinearRegression`. For numerical reasons, using\n",
      "     |      ``alpha = 0`` with the LassoLars object is not advised and you\n",
      "     |      should prefer the LinearRegression object.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like, default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |      By default, ``np.finfo(np.float).eps`` is used.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  fit_path : bool, default=True\n",
      "     |      If ``True`` the full path is stored in the ``coef_path_`` attribute.\n",
      "     |      If you compute the solution for a large problem or many targets,\n",
      "     |      setting ``fit_path`` to ``False`` will lead to a speedup, especially\n",
      "     |      with a small alpha.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients will not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |  \n",
      "     |  jitter : float, default=None\n",
      "     |      Upper bound on a uniform noise parameter to be added to the\n",
      "     |      `y` values, to satisfy the model's assumption of\n",
      "     |      one-at-a-time computations. Might help with stability.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None (default)\n",
      "     |      Determines random number generation for jittering. Pass an int\n",
      "     |      for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alphas_ : array-like of shape (n_alphas + 1,) | list of n_targets such             arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``max_iter``, ``n_features``, or the number of         nodes in the path with correlation greater than ``alpha``, whichever         is smaller.\n",
      "     |  \n",
      "     |  active_ : list, length = n_alphas | list of n_targets such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas + 1) or list\n",
      "     |      If a list is passed it's expected to be one of n_targets such arrays.\n",
      "     |      The varying values of the coefficients along the path. It is not\n",
      "     |      present if the ``fit_path`` parameter is ``False``.\n",
      "     |  \n",
      "     |  coef_ : array-like of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formulation formula).\n",
      "     |  \n",
      "     |  intercept_ : float or array-like of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int.\n",
      "     |      The number of iterations taken by lars_path to find the\n",
      "     |      grid of alphas for each target.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.LassoLars(alpha=0.01)\n",
      "     |  >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\n",
      "     |  LassoLars(alpha=0.01)\n",
      "     |  >>> print(reg.coef_)\n",
      "     |  [ 0.         -0.963257...]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path\n",
      "     |  lasso_path\n",
      "     |  Lasso\n",
      "     |  LassoCV\n",
      "     |  LassoLarsCV\n",
      "     |  LassoLarsIC\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLars\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, fit_path=True, positive=False, jitter=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Lars:\n",
      "     |  \n",
      "     |  fit(self, X, y, Xy=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_samples,) or (n_samples, n_targets),                 default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LassoLarsCV(LarsCV)\n",
      "     |  LassoLarsCV(*, fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True, positive=False)\n",
      "     |  \n",
      "     |  Cross-validated Lasso, using the LARS algorithm.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : bool or 'auto' , default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram matrix\n",
      "     |      cannot be passed as argument since we will use only subsets of X.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  max_n_alphas : int, default=1000\n",
      "     |      The maximum number of points on the path used to compute the\n",
      "     |      residuals in the cross-validation\n",
      "     |  \n",
      "     |  n_jobs : int or None, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. By default, ``np.finfo(np.float).eps`` is used.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients do not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |      As a consequence using LassoLarsCV only makes sense for problems where\n",
      "     |      a sparse solution is expected and/or reached.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas)\n",
      "     |      the varying values of the coefficients along the path\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the estimated regularization parameter alpha\n",
      "     |  \n",
      "     |  alphas_ : array-like of shape (n_alphas,)\n",
      "     |      the different values of alpha along the path\n",
      "     |  \n",
      "     |  cv_alphas_ : array-like of shape (n_cv_alphas,)\n",
      "     |      all the values of alpha along the path for the different folds\n",
      "     |  \n",
      "     |  mse_path_ : array-like of shape (n_folds, n_cv_alphas)\n",
      "     |      the mean square error on left-out for each fold along the path\n",
      "     |      (alpha values given by ``cv_alphas``)\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      the number of iterations run by Lars with the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import LassoLarsCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(noise=4.0, random_state=0)\n",
      "     |  >>> reg = LassoLarsCV(cv=5).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9992...\n",
      "     |  >>> reg.alpha_\n",
      "     |  0.0484...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-77.8723...])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  \n",
      "     |  The object solves the same problem as the LassoCV object. However,\n",
      "     |  unlike the LassoCV, it find the relevant alphas values by itself.\n",
      "     |  In general, because of this property, it will be more stable.\n",
      "     |  However, it is more fragile to heavily multicollinear datasets.\n",
      "     |  \n",
      "     |  It is more efficient than the LassoCV if only a small number of\n",
      "     |  features are selected compared to the total number, for instance if\n",
      "     |  there are very few samples compared to the number of features.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path, LassoLars, LarsCV, LassoCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLarsCV\n",
      "     |      LarsCV\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=2.220446049250313e-16, copy_X=True, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LarsCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LassoLarsIC(LassoLars)\n",
      "     |  LassoLarsIC(criterion='aic', *, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, positive=False)\n",
      "     |  \n",
      "     |  Lasso model fit with Lars using BIC or AIC for model selection\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  AIC is the Akaike information criterion and BIC is the Bayes\n",
      "     |  Information criterion. Such criteria are useful to select the value\n",
      "     |  of the regularization parameter by making a trade-off between the\n",
      "     |  goodness of fit and the complexity of the model. A good model should\n",
      "     |  explain well the data while being simple.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  criterion : {'bic' , 'aic'}, default='aic'\n",
      "     |      The type of criterion to use.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : bool, default=True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like, default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform. Can be used for\n",
      "     |      early stopping.\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |      By default, ``np.finfo(np.float).eps`` is used\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients do not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |      As a consequence using LassoLarsIC only makes sense for problems where\n",
      "     |      a sparse solution is expected and/or reached.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the alpha parameter chosen by the information criterion\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by lars_path to find the grid of\n",
      "     |      alphas.\n",
      "     |  \n",
      "     |  criterion_ : array-like of shape (n_alphas,)\n",
      "     |      The value of the information criteria ('aic', 'bic') across all\n",
      "     |      alphas. The alpha which has the smallest information criterion is\n",
      "     |      chosen. This value is larger by a factor of ``n_samples`` compared to\n",
      "     |      Eqns. 2.15 and 2.16 in (Zou et al, 2007).\n",
      "     |  \n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.LassoLarsIC(criterion='bic')\n",
      "     |  >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n",
      "     |  LassoLarsIC(criterion='bic')\n",
      "     |  >>> print(reg.coef_)\n",
      "     |  [ 0.  -1.11...]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The estimation of the number of degrees of freedom is given by:\n",
      "     |  \n",
      "     |  \"On the degrees of freedom of the lasso\"\n",
      "     |  Hui Zou, Trevor Hastie, and Robert Tibshirani\n",
      "     |  Ann. Statist. Volume 35, Number 5 (2007), 2173-2192.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/Akaike_information_criterion\n",
      "     |  https://en.wikipedia.org/wiki/Bayesian_information_criterion\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path, LassoLars, LassoLarsCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLarsIC\n",
      "     |      LassoLars\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, criterion='aic', *, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.220446049250313e-16, copy_X=True, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, copy_X=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      copy_X : bool, default=None\n",
      "     |          If provided, this parameter will override the choice\n",
      "     |          of copy_X made at instance creation.\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from LassoLars:\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, LinearModel)\n",
      "     |  LinearRegression(*, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
      "     |  \n",
      "     |  Ordinary least squares Linear Regression.\n",
      "     |  \n",
      "     |  LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
      "     |  to minimize the residual sum of squares between the observed targets in\n",
      "     |  the dataset, and the targets predicted by the linear approximation.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to False, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on\n",
      "     |      an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to use for the computation. This will only provide\n",
      "     |      speedup for n_targets > 1 and sufficient large problems.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
      "     |      Estimated coefficients for the linear regression problem.\n",
      "     |      If multiple targets are passed during the fit (y 2D), this\n",
      "     |      is a 2D array of shape (n_targets, n_features), while if only\n",
      "     |      one target is passed, this is a 1D array of length n_features.\n",
      "     |  \n",
      "     |  rank_ : int\n",
      "     |      Rank of matrix `X`. Only available when `X` is dense.\n",
      "     |  \n",
      "     |  singular_ : array of shape (min(X, y),)\n",
      "     |      Singular values of `X`. Only available when `X` is dense.\n",
      "     |  \n",
      "     |  intercept_ : float or array of shape (n_targets,)\n",
      "     |      Independent term in the linear model. Set to 0.0 if\n",
      "     |      `fit_intercept = False`.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.linear_model.Ridge : Ridge regression addresses some of the\n",
      "     |      problems of Ordinary Least Squares by imposing a penalty on the\n",
      "     |      size of the coefficients with l2 regularization.\n",
      "     |  sklearn.linear_model.Lasso : The Lasso is a linear model that estimates\n",
      "     |      sparse coefficients with l1 regularization.\n",
      "     |  sklearn.linear_model.ElasticNet : Elastic-Net is a linear regression\n",
      "     |      model trained with both l1 and l2 -norm regularization of the\n",
      "     |      coefficients.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  From the implementation point of view, this is just plain Ordinary\n",
      "     |  Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import LinearRegression\n",
      "     |  >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
      "     |  >>> # y = 1 * x_0 + 2 * x_1 + 3\n",
      "     |  >>> y = np.dot(X, np.array([1, 2])) + 3\n",
      "     |  >>> reg = LinearRegression().fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  1.0\n",
      "     |  >>> reg.coef_\n",
      "     |  array([1., 2.])\n",
      "     |  >>> reg.intercept_\n",
      "     |  3.0000...\n",
      "     |  >>> reg.predict(np.array([[3, 5]]))\n",
      "     |  array([16.])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearRegression\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             parameter *sample_weight* support to LinearRegression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Log(Classification)\n",
      "     |  Logistic regression loss for binary classification with y in {-1, 1}\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Log\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Classification:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin)\n",
      "     |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      "     |  \n",
      "     |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      "     |  \n",
      "     |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      "     |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      "     |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      "     |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      "     |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      "     |  \n",
      "     |  This class implements regularized logistic regression using the\n",
      "     |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      "     |  that regularization is applied by default**. It can handle both dense\n",
      "     |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      "     |  floats for optimal performance; any other input format will be converted\n",
      "     |  (and copied).\n",
      "     |  \n",
      "     |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      "     |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      "     |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      "     |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      "     |  'saga' solver.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      "     |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      "     |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      "     |      only supported by the 'saga' solver. If 'none' (not supported by the\n",
      "     |      liblinear solver), no regularization is applied.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      "     |  \n",
      "     |  dual : bool, default=False\n",
      "     |      Dual or primal formulation. Dual formulation is only implemented for\n",
      "     |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      "     |      n_samples > n_features.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  C : float, default=1.0\n",
      "     |      Inverse of regularization strength; must be a positive float.\n",
      "     |      Like in support vector machines, smaller values specify stronger\n",
      "     |      regularization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the decision function.\n",
      "     |  \n",
      "     |  intercept_scaling : float, default=1\n",
      "     |      Useful only when the solver 'liblinear' is used\n",
      "     |      and self.fit_intercept is set to True. In this case, x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equal to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "     |  \n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *class_weight='balanced'*\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      "     |      data. See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      "     |  \n",
      "     |      Algorithm to use in the optimization problem.\n",
      "     |  \n",
      "     |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      "     |        'saga' are faster for large ones.\n",
      "     |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      "     |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      "     |        schemes.\n",
      "     |      - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
      "     |      - 'liblinear' and 'saga' also handle L1 penalty\n",
      "     |      - 'saga' also supports 'elasticnet' penalty\n",
      "     |      - 'liblinear' does not support setting ``penalty='none'``\n",
      "     |  \n",
      "     |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      "     |      features with approximately the same scale. You can\n",
      "     |      preprocess the data with a scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      Maximum number of iterations taken for the solvers to converge.\n",
      "     |  \n",
      "     |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      "     |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      "     |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      "     |      across the entire probability distribution, *even when the data is\n",
      "     |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      "     |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      "     |      and otherwise selects 'multinomial'.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      "     |      number for verbosity.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPU cores used when parallelizing over classes if\n",
      "     |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      "     |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      "     |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors.\n",
      "     |      See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=None\n",
      "     |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      "     |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      "     |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      "     |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      "     |      combination of L1 and L2.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes, )\n",
      "     |      A list of class labels known to the classifier.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      "     |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      "     |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      "     |      Intercept (a.k.a. bias) added to the decision function.\n",
      "     |  \n",
      "     |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "     |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      "     |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      "     |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      "     |      outcome 0 (False).\n",
      "     |  \n",
      "     |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      "     |      Actual number of iterations for all classes. If binary or multinomial,\n",
      "     |      it returns only 1 element. For liblinear solver, only the maximum\n",
      "     |      number of iteration across all classes is given.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |  \n",
      "     |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      "     |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      "     |      the parameter ``loss=\"log\"``).\n",
      "     |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The underlying C implementation uses a random number generator to\n",
      "     |  select features when fitting the model. It is thus not uncommon,\n",
      "     |  to have slightly different results for the same input data. If\n",
      "     |  that happens, try with a smaller tol parameter.\n",
      "     |  \n",
      "     |  Predict output may not match that of standalone liblinear in certain\n",
      "     |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "     |  in the narrative documentation.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      "     |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      "     |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      "     |  \n",
      "     |  LIBLINEAR -- A Library for Large Linear Classification\n",
      "     |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      "     |  \n",
      "     |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      "     |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      "     |      https://hal.inria.fr/hal-00860051/document\n",
      "     |  \n",
      "     |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      "     |      SAGA: A Fast Incremental Gradient Method With Support\n",
      "     |      for Non-Strongly Convex Composite Objectives\n",
      "     |      https://arxiv.org/abs/1407.0202\n",
      "     |  \n",
      "     |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      "     |      methods for logistic regression and maximum entropy models.\n",
      "     |      Machine Learning 85(1-2):41-75.\n",
      "     |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      "     |  >>> clf.predict(X[:2, :])\n",
      "     |  array([0, 0])\n",
      "     |  >>> clf.predict_proba(X[:2, :])\n",
      "     |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      "     |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.97...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticRegression\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,) default=None\n",
      "     |          Array of weights that are assigned to individual samples.\n",
      "     |          If not provided, then each sample is given unit weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             *sample_weight* support to LogisticRegression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict logarithm of probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      "     |      the softmax function is used to find the predicted probability of\n",
      "     |      each class.\n",
      "     |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      "     |      of each class assuming it to be positive using the logistic function.\n",
      "     |      and normalize these values across all the classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "    \n",
      "    class LogisticRegressionCV(LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model._base.LinearClassifierMixin)\n",
      "     |  LogisticRegressionCV(*, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='auto', random_state=None, l1_ratios=None)\n",
      "     |  \n",
      "     |  Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  This class implements logistic regression using liblinear, newton-cg, sag\n",
      "     |  of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n",
      "     |  regularization with primal formulation. The liblinear solver supports both\n",
      "     |  L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n",
      "     |  Elastic-Net penalty is only supported by the saga solver.\n",
      "     |  \n",
      "     |  For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n",
      "     |  is selected by the cross-validator\n",
      "     |  :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n",
      "     |  using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      "     |  solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  Cs : int or list of floats, default=10\n",
      "     |      Each of the values in Cs describes the inverse of regularization\n",
      "     |      strength. If Cs is as an int, then a grid of Cs values are chosen\n",
      "     |      in a logarithmic scale between 1e-4 and 1e4.\n",
      "     |      Like in support vector machines, smaller values specify stronger\n",
      "     |      regularization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the decision function.\n",
      "     |  \n",
      "     |  cv : int or cross-validation generator, default=None\n",
      "     |      The default cross-validation generator used is Stratified K-Folds.\n",
      "     |      If an integer is provided, then it is the number of folds used.\n",
      "     |      See the module :mod:`sklearn.model_selection` module for the\n",
      "     |      list of possible cross-validation objects.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  dual : bool, default=False\n",
      "     |      Dual or primal formulation. Dual formulation is only implemented for\n",
      "     |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      "     |      n_samples > n_features.\n",
      "     |  \n",
      "     |  penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n",
      "     |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      "     |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      "     |      only supported by the 'saga' solver.\n",
      "     |  \n",
      "     |  scoring : str or callable, default=None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``. For a list of scoring functions\n",
      "     |      that can be used, look at :mod:`sklearn.metrics`. The\n",
      "     |      default scoring option used is 'accuracy'.\n",
      "     |  \n",
      "     |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      "     |  \n",
      "     |      Algorithm to use in the optimization problem.\n",
      "     |  \n",
      "     |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      "     |        'saga' are faster for large ones.\n",
      "     |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      "     |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      "     |        schemes.\n",
      "     |      - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
      "     |        'liblinear' and 'saga' handle L1 penalty.\n",
      "     |      - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
      "     |        not handle warm-starting.\n",
      "     |  \n",
      "     |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      "     |      features with approximately the same scale. You can preprocess the data\n",
      "     |      with a scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      Maximum number of iterations of the optimization algorithm.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         class_weight == 'balanced'\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPU cores used during the cross-validation loop.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n",
      "     |      positive number for verbosity.\n",
      "     |  \n",
      "     |  refit : bool, default=True\n",
      "     |      If set to True, the scores are averaged across all folds, and the\n",
      "     |      coefs and the C that corresponds to the best score is taken, and a\n",
      "     |      final refit is done using these parameters.\n",
      "     |      Otherwise the coefs, intercepts and C that correspond to the\n",
      "     |      best scores across folds are averaged.\n",
      "     |  \n",
      "     |  intercept_scaling : float, default=1\n",
      "     |      Useful only when the solver 'liblinear' is used\n",
      "     |      and self.fit_intercept is set to True. In this case, x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equal to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "     |  \n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n",
      "     |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      "     |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      "     |      across the entire probability distribution, *even when the data is\n",
      "     |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      "     |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      "     |      and otherwise selects 'multinomial'.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n",
      "     |      Note that this only applies to the solver and not the cross-validation\n",
      "     |      generator. See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  l1_ratios : list of float, default=None\n",
      "     |      The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n",
      "     |      Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n",
      "     |      using ``penalty='l2'``, while 1 is equivalent to using\n",
      "     |      ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n",
      "     |      of L1 and L2.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes, )\n",
      "     |      A list of class labels known to the classifier.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      `coef_` is of shape (1, n_features) when the given problem\n",
      "     |      is binary.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      "     |      Intercept (a.k.a. bias) added to the decision function.\n",
      "     |  \n",
      "     |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "     |      `intercept_` is of shape(1,) when the problem is binary.\n",
      "     |  \n",
      "     |  Cs_ : ndarray of shape (n_cs)\n",
      "     |      Array of C i.e. inverse of regularization parameter values used\n",
      "     |      for cross-validation.\n",
      "     |  \n",
      "     |  l1_ratios_ : ndarray of shape (n_l1_ratios)\n",
      "     |      Array of l1_ratios used for cross-validation. If no l1_ratio is used\n",
      "     |      (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n",
      "     |  \n",
      "     |  coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\n",
      "     |      dict with classes as the keys, and the path of coefficients obtained\n",
      "     |      during cross-validating across each fold and then across each Cs\n",
      "     |      after doing an OvR for the corresponding class as values.\n",
      "     |      If the 'multi_class' option is set to 'multinomial', then\n",
      "     |      the coefs_paths are the coefficients corresponding to each class.\n",
      "     |      Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n",
      "     |      ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n",
      "     |      intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n",
      "     |      ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n",
      "     |      ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n",
      "     |  \n",
      "     |  scores_ : dict\n",
      "     |      dict with classes as the keys, and the values as the\n",
      "     |      grid of scores obtained during cross-validating each fold, after doing\n",
      "     |      an OvR for the corresponding class. If the 'multi_class' option\n",
      "     |      given is 'multinomial' then the same scores are repeated across\n",
      "     |      all classes, since this is the multinomial class. Each dict value\n",
      "     |      has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n",
      "     |      ``penalty='elasticnet'``.\n",
      "     |  \n",
      "     |  C_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
      "     |      Array of C that maps to the best scores across every class. If refit is\n",
      "     |      set to False, then for each class, the best C is the average of the\n",
      "     |      C's that correspond to the best scores for each fold.\n",
      "     |      `C_` is of shape(n_classes,) when the problem is binary.\n",
      "     |  \n",
      "     |  l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
      "     |      Array of l1_ratio that maps to the best scores across every class. If\n",
      "     |      refit is set to False, then for each class, the best l1_ratio is the\n",
      "     |      average of the l1_ratio's that correspond to the best scores for each\n",
      "     |      fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n",
      "     |  \n",
      "     |  n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n",
      "     |      Actual number of iterations for all classes, folds and Cs.\n",
      "     |      In the binary or multinomial cases, the first dimension is equal to 1.\n",
      "     |      If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n",
      "     |      n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n",
      "     |  \n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.linear_model import LogisticRegressionCV\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
      "     |  >>> clf.predict(X[:2, :])\n",
      "     |  array([0, 0])\n",
      "     |  >>> clf.predict_proba(X[:2, :]).shape\n",
      "     |  (2, 3)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.98...\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  LogisticRegression\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticRegressionCV\n",
      "     |      LogisticRegression\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='auto', random_state=None, l1_ratios=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,) default=None\n",
      "     |          Array of weights that are assigned to individual samples.\n",
      "     |          If not provided, then each sample is given unit weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the score using the `scoring` option on the given\n",
      "     |      test data and labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Score of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LogisticRegression:\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict logarithm of probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      "     |      the softmax function is used to find the predicted probability of\n",
      "     |      each class.\n",
      "     |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      "     |      of each class assuming it to be positive using the logistic function.\n",
      "     |      and normalize these values across all the classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "    \n",
      "    class ModifiedHuber(Classification)\n",
      "     |  Modified Huber loss for binary classification with y in {-1, 1}\n",
      "     |  \n",
      "     |  This is equivalent to quadratically smoothed SVM with gamma = 2.\n",
      "     |  \n",
      "     |  See T. Zhang 'Solving Large Scale Linear Prediction Problems Using\n",
      "     |  Stochastic Gradient Descent', ICML'04.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ModifiedHuber\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Classification:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class MultiTaskElasticNet(Lasso)\n",
      "     |  MultiTaskElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskElasticNet is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |      + alpha * l1_ratio * ||W||_21\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)\n",
      "     |  \n",
      "     |  i.e. the sum of norms of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the L1/L2 term. Defaults to 1.0\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.5\n",
      "     |      The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n",
      "     |      For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n",
      "     |      is an L2 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : ndarray of shape (n_tasks,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula). If a 1D y is\n",
      "     |      passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n",
      "     |  MultiTaskElasticNet(alpha=0.1)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.45663524 0.45612256]\n",
      "     |   [0.45663524 0.45612256]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [0.0872422 0.0872422]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNet : Multi-task L1/L2 ElasticNet with built-in\n",
      "     |      cross-validation.\n",
      "     |  ElasticNet\n",
      "     |  MultiTaskLasso\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskElasticNet\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskElasticNet model with coordinate descent\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Data\n",
      "     |      y : ndarray of shape (n_samples, n_tasks)\n",
      "     |          Target. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Lasso:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      sparse representation of the fitted ``coef_``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MultiTaskElasticNetCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  MultiTaskElasticNetCV(*, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskElasticNet is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |      + alpha * l1_ratio * ||W||_21\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_elastic_net>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.15\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  l1_ratio : float or list of float, default=0.5\n",
      "     |      The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n",
      "     |      For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n",
      "     |      is an L2 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n",
      "     |      This parameter can be a list, in which case the different\n",
      "     |      values are tested by cross-validation and the one giving the best\n",
      "     |      prediction score is used. Note that a good choice of list of\n",
      "     |      values for l1_ratio is often to put more values close to 1\n",
      "     |      (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n",
      "     |      .9, .95, .99, 1]``\n",
      "     |  \n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path\n",
      "     |  \n",
      "     |  alphas : array-like, default=None\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If not provided, set automatically.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=0\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation. Note that this is\n",
      "     |      used only if multiple values for l1_ratio are given.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : ndarray of shape (n_tasks,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)\n",
      "     |      mean square error for the test set on each fold, varying alpha\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n",
      "     |      The grid of alphas used for fitting, for each l1_ratio\n",
      "     |  \n",
      "     |  l1_ratio_ : float\n",
      "     |      best l1_ratio obtained by cross-validation.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskElasticNetCV(cv=3)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]],\n",
      "     |  ...         [[0, 0], [1, 1], [2, 2]])\n",
      "     |  MultiTaskElasticNetCV(cv=3)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.52875032 0.46958558]\n",
      "     |   [0.52875032 0.46958558]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [0.00166409 0.00166409]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNet\n",
      "     |  ElasticNetCV\n",
      "     |  MultiTaskLassoCV\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MultiTaskLasso(MultiTaskElasticNet)\n",
      "     |  MultiTaskLasso(alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the L1/L2 term. Defaults to 1.0\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (n_tasks,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskLasso(alpha=0.1)\n",
      "     |  >>> clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])\n",
      "     |  MultiTaskLasso(alpha=0.1)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.         0.60809415]\n",
      "     |  [0.         0.94592424]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [-0.41888636 -0.87382323]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  MultiTaskLasso : Multi-task L1/L2 Lasso with built-in cross-validation\n",
      "     |  Lasso\n",
      "     |  MultiTaskElasticNet\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskLasso\n",
      "     |      MultiTaskElasticNet\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MultiTaskElasticNet:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskElasticNet model with coordinate descent\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Data\n",
      "     |      y : ndarray of shape (n_samples, n_tasks)\n",
      "     |          Target. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Lasso:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      sparse representation of the fitted ``coef_``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class MultiTaskLassoCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  MultiTaskLassoCV(*, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskLasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.15\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path\n",
      "     |  \n",
      "     |  alphas : array-like, default=None\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If not provided, set automatically.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation. Note that this is\n",
      "     |      used only if multiple values for l1_ratio are given.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : ndarray of shape (n_tasks,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_alphas, n_folds)\n",
      "     |      mean square error for the test set on each fold, varying alpha\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,)\n",
      "     |      The grid of alphas used for fitting.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import MultiTaskLassoCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> from sklearn.metrics import r2_score\n",
      "     |  >>> X, y = make_regression(n_targets=2, noise=4, random_state=0)\n",
      "     |  >>> reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\n",
      "     |  >>> r2_score(y, reg.predict(X))\n",
      "     |  0.9994...\n",
      "     |  >>> reg.alpha_\n",
      "     |  0.5713...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([[153.7971...,  94.9015...]])\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNet\n",
      "     |  ElasticNetCV\n",
      "     |  MultiTaskElasticNetCV\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskLassoCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "     |      Compute Lasso path with coordinate descent\n",
      "     |      \n",
      "     |      The Lasso optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <lasso>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If ``None`` alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |      should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |      \n",
      "     |      Note that in certain cases, the Lars solver may be significantly\n",
      "     |      faster to implement this functionality. In particular, linear\n",
      "     |      interpolation can be used to retrieve model coefficients between the\n",
      "     |      values output by lars_path\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      Comparing lasso_path and lars_path with interpolation:\n",
      "     |      \n",
      "     |      >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "     |      >>> y = np.array([1, 2, 3.1])\n",
      "     |      >>> # Use lasso_path to compute a coefficient path\n",
      "     |      >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "     |      >>> print(coef_path)\n",
      "     |      [[0.         0.         0.46874778]\n",
      "     |       [0.2159048  0.4425765  0.23689075]]\n",
      "     |      \n",
      "     |      >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "     |      >>> # same path\n",
      "     |      >>> from sklearn.linear_model import lars_path\n",
      "     |      >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "     |      >>> from scipy import interpolate\n",
      "     |      >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "     |      ...                                             coef_path_lars[:, ::-1])\n",
      "     |      >>> print(coef_path_continuous([5., 1., .5]))\n",
      "     |      [[0.         0.         0.46915237]\n",
      "     |       [0.2159048  0.4425765  0.23668876]]\n",
      "     |      \n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      lars_path\n",
      "     |      Lasso\n",
      "     |      LassoLars\n",
      "     |      LassoCV\n",
      "     |      LassoLarsCV\n",
      "     |      sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  OrthogonalMatchingPursuit(*, n_nonzero_coefs=None, tol=None, fit_intercept=True, normalize=True, precompute='auto')\n",
      "     |  \n",
      "     |  Orthogonal Matching Pursuit model (OMP)\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <omp>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_nonzero_coefs : int, optional\n",
      "     |      Desired number of non-zero entries in the solution. If None (by\n",
      "     |      default) this value is set to 10% of n_features.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : {True, False, 'auto'}, default 'auto'\n",
      "     |      Whether to use a precomputed Gram and Xy matrix to speed up\n",
      "     |      calculations. Improves performance when :term:`n_targets` or\n",
      "     |      :term:`n_samples` is very large. Note that if you already have such\n",
      "     |      matrices, you can pass them directly to the fit method.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
      "     |      parameter vector (w in the formula)\n",
      "     |  \n",
      "     |  intercept_ : float or array, shape (n_targets,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int or array-like\n",
      "     |      Number of active features across every target.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import OrthogonalMatchingPursuit\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(noise=4, random_state=0)\n",
      "     |  >>> reg = OrthogonalMatchingPursuit().fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9991...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.3854...])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n",
      "     |  Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "     |  Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "     |  (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "     |  \n",
      "     |  This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "     |  M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "     |  Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "     |  https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  orthogonal_mp\n",
      "     |  orthogonal_mp_gram\n",
      "     |  lars_path\n",
      "     |  Lars\n",
      "     |  LassoLars\n",
      "     |  decomposition.sparse_encode\n",
      "     |  OrthogonalMatchingPursuitCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrthogonalMatchingPursuit\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_nonzero_coefs=None, tol=None, fit_intercept=True, normalize=True, precompute='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class OrthogonalMatchingPursuitCV(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  OrthogonalMatchingPursuitCV(*, copy=True, fit_intercept=True, normalize=True, max_iter=None, cv=None, n_jobs=None, verbose=False)\n",
      "     |  \n",
      "     |  Cross-validated Orthogonal Matching Pursuit model (OMP).\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <omp>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  copy : bool, optional\n",
      "     |      Whether the design matrix X must be copied by the algorithm. A false\n",
      "     |      value is only helpful if X is already Fortran-ordered, otherwise a\n",
      "     |      copy is made anyway.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  max_iter : integer, optional\n",
      "     |      Maximum numbers of iterations to perform, therefore maximum features\n",
      "     |      to include. 10% of ``n_features`` but at least 5 if available.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : float or array, shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the problem formulation).\n",
      "     |  \n",
      "     |  n_nonzero_coefs_ : int\n",
      "     |      Estimated number of non-zero coefficients giving the best mean squared\n",
      "     |      error over the cross-validation folds.\n",
      "     |  \n",
      "     |  n_iter_ : int or array-like\n",
      "     |      Number of active features across every target for the model refit with\n",
      "     |      the best hyperparameters got by cross-validating across all folds.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import OrthogonalMatchingPursuitCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_features=100, n_informative=10,\n",
      "     |  ...                        noise=4, random_state=0)\n",
      "     |  >>> reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9991...\n",
      "     |  >>> reg.n_nonzero_coefs_\n",
      "     |  10\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.3854...])\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  orthogonal_mp\n",
      "     |  orthogonal_mp_gram\n",
      "     |  lars_path\n",
      "     |  Lars\n",
      "     |  LassoLars\n",
      "     |  OrthogonalMatchingPursuit\n",
      "     |  LarsCV\n",
      "     |  LassoLarsCV\n",
      "     |  decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrthogonalMatchingPursuitCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, copy=True, fit_intercept=True, normalize=True, max_iter=None, cv=None, n_jobs=None, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape [n_samples]\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class PassiveAggressiveClassifier(sklearn.linear_model._stochastic_gradient.BaseSGDClassifier)\n",
      "     |  PassiveAggressiveClassifier(*, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='hinge', n_jobs=None, random_state=None, warm_start=False, class_weight=None, average=False)\n",
      "     |  \n",
      "     |  Passive Aggressive Classifier\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <passive_aggressive>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  C : float\n",
      "     |      Maximum step size (regularization). Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=False\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, optional (default=1000)\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, optional (default=1e-3)\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol).\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation.\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a stratified fraction of training data as validation and terminate\n",
      "     |      training when validation score is not improving by at least tol for\n",
      "     |      n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : integer, optional\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  loss : string, optional\n",
      "     |      The loss function to be used:\n",
      "     |      hinge: equivalent to PA-I in the reference paper.\n",
      "     |      squared_hinge: equivalent to PA-II in the reference paper.\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used to shuffle the training data, when ``shuffle`` is set to\n",
      "     |      ``True``. Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\" or None, optional\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         parameter *class_weight* to automatically weight samples.\n",
      "     |  \n",
      "     |  average : bool or int, optional\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So average=10 will begin averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         parameter *average* to use weights averaging in SGD\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  classes_ : array of shape (n_classes,)\n",
      "     |      The unique classes labels.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  loss_function_ : callable\n",
      "     |      Loss function used by the algorithm.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import PassiveAggressiveClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  \n",
      "     |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      "     |  >>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,\n",
      "     |  ... tol=1e-3)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  PassiveAggressiveClassifier(random_state=0)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.26642044 0.45070924 0.67251877 0.64185414]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [1.84127814]\n",
      "     |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  \n",
      "     |  SGDClassifier\n",
      "     |  Perceptron\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Online Passive-Aggressive Algorithms\n",
      "     |  <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n",
      "     |  K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PassiveAggressiveClassifier\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGDClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='hinge', n_jobs=None, random_state=None, warm_start=False, class_weight=None, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape = [n_classes,n_features]\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape = [n_classes]\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Subset of the training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Subset of the target values\n",
      "     |      \n",
      "     |      classes : array, shape = [n_classes]\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, **kwargs)\n",
      "     |      Set and validate the parameters of estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  average_coef_\n",
      "     |  \n",
      "     |  average_intercept_\n",
      "     |  \n",
      "     |  standard_coef_\n",
      "     |  \n",
      "     |  standard_intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "    \n",
      "    class PassiveAggressiveRegressor(sklearn.linear_model._stochastic_gradient.BaseSGDRegressor)\n",
      "     |  PassiveAggressiveRegressor(*, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='epsilon_insensitive', epsilon=0.1, random_state=None, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Passive Aggressive Regressor\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <passive_aggressive>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  C : float\n",
      "     |      Maximum step size (regularization). Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered. Defaults to True.\n",
      "     |  \n",
      "     |  max_iter : int, optional (default=1000)\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, optional (default=1e-3)\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol).\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation.\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a fraction of training data as validation and terminate\n",
      "     |      training when validation score is not improving by at least tol for\n",
      "     |      n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : integer, optional\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  loss : string, optional\n",
      "     |      The loss function to be used:\n",
      "     |      epsilon_insensitive: equivalent to PA-I in the reference paper.\n",
      "     |      squared_epsilon_insensitive: equivalent to PA-II in the reference\n",
      "     |      paper.\n",
      "     |  \n",
      "     |  epsilon : float\n",
      "     |      If the difference between the current prediction and the correct label\n",
      "     |      is below this threshold, the model is not updated.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used to shuffle the training data, when ``shuffle`` is set to\n",
      "     |      ``True``. Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |  \n",
      "     |  average : bool or int, optional\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So average=10 will begin averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         parameter *average* to use weights averaging in SGD\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import PassiveAggressiveRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  \n",
      "     |  >>> X, y = make_regression(n_features=4, random_state=0)\n",
      "     |  >>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\n",
      "     |  ... tol=1e-3)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  PassiveAggressiveRegressor(max_iter=100, random_state=0)\n",
      "     |  >>> print(regr.coef_)\n",
      "     |  [20.48736655 34.18818427 67.59122734 87.94731329]\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  [-0.02306214]\n",
      "     |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "     |  [-0.02306214]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  \n",
      "     |  SGDRegressor\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Online Passive-Aggressive Algorithms\n",
      "     |  <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n",
      "     |  K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PassiveAggressiveRegressor\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGDRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='epsilon_insensitive', epsilon=0.1, random_state=None, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape = [n_features]\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape = [1]\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Subset of training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Subset of target values\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGDRegressor:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ndarray of shape (n_samples,)\n",
      "     |         Predicted target values per element in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model._stochastic_gradient.BaseSGDRegressor:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, **kwargs)\n",
      "     |      Set and validate the parameters of estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  average_coef_\n",
      "     |  \n",
      "     |  average_intercept_\n",
      "     |  \n",
      "     |  standard_coef_\n",
      "     |  \n",
      "     |  standard_intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "    \n",
      "    class Perceptron(sklearn.linear_model._stochastic_gradient.BaseSGDClassifier)\n",
      "     |  Perceptron(*, penalty=None, alpha=0.0001, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False)\n",
      "     |  \n",
      "     |  Perceptron\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <perceptron>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  penalty : {'l2','l1','elasticnet'}, default=None\n",
      "     |      The penalty (aka regularization term) to be used.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Constant that multiplies the regularization term if regularization is\n",
      "     |      used.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol).\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  eta0 : double, default=1\n",
      "     |      Constant by which the updates are multiplied.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used to shuffle the training data, when ``shuffle`` is set to\n",
      "     |      ``True``. Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation.\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a stratified fraction of training data as validation and terminate\n",
      "     |      training when validation score is not improving by at least tol for\n",
      "     |      n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution. See\n",
      "     |      :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape = [1, n_features] if n_classes == 2 else         [n_classes, n_features]\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The unique classes labels.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  \n",
      "     |  ``Perceptron`` is a classification algorithm which shares the same\n",
      "     |  underlying implementation with ``SGDClassifier``. In fact,\n",
      "     |  ``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",\n",
      "     |  eta0=1, learning_rate=\"constant\", penalty=None)`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_digits\n",
      "     |  >>> from sklearn.linear_model import Perceptron\n",
      "     |  >>> X, y = load_digits(return_X_y=True)\n",
      "     |  >>> clf = Perceptron(tol=1e-3, random_state=0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  Perceptron()\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.939...\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  \n",
      "     |  SGDClassifier\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/Perceptron and references therein.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Perceptron\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGDClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, penalty=None, alpha=0.0001, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_classes, n_features), default=None\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (n_classes,), default=None\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self :\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Perform one epoch of stochastic gradient descent on given samples.\n",
      "     |      \n",
      "     |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      "     |      guaranteed that a minimum of the cost function is reached after calling\n",
      "     |      it once. Matters such as objective convergence and early stopping\n",
      "     |      should be handled by the user.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Subset of the target values.\n",
      "     |      \n",
      "     |      classes : ndarray of shape (n_classes,), default=None\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self :\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, **kwargs)\n",
      "     |      Set and validate the parameters of estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  average_coef_\n",
      "     |  \n",
      "     |  average_intercept_\n",
      "     |  \n",
      "     |  standard_coef_\n",
      "     |  \n",
      "     |  standard_intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "    \n",
      "    class PoissonRegressor(GeneralizedLinearRegressor)\n",
      "     |  PoissonRegressor(*, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |  \n",
      "     |  Generalized Linear Model with a Poisson distribution.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <Generalized_linear_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1\n",
      "     |      Constant that multiplies the penalty term and thus determines the\n",
      "     |      regularization strength. ``alpha = 0`` is equivalent to unpenalized\n",
      "     |      GLMs. In this case, the design matrix `X` must have full column rank\n",
      "     |      (no collinearities).\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the linear predictor (X @ coef + intercept).\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximal number of iterations for the solver.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Stopping criterion. For the lbfgs solver,\n",
      "     |      the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n",
      "     |      where ``g_j`` is the j-th component of the gradient (derivative) of\n",
      "     |      the objective function.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If set to ``True``, reuse the solution of the previous call to ``fit``\n",
      "     |      as initialization for ``coef_`` and ``intercept_`` .\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the lbfgs solver set verbose to any positive number for verbosity.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the linear predictor (`X @ coef_ +\n",
      "     |      intercept_`) in the GLM.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Intercept (a.k.a. bias) added to linear predictor.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Actual number of iterations used in the solver.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PoissonRegressor\n",
      "     |      GeneralizedLinearRegressor\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  family\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from GeneralizedLinearRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit a Generalized Linear Model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using GLM with feature matrix X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array of shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Compute D^2, the percentage of deviance explained.\n",
      "     |      \n",
      "     |      D^2 is a generalization of the coefficient of determination R^2.\n",
      "     |      R^2 uses squared error and D^2 deviance. Note that those two are equal\n",
      "     |      for ``family='normal'``.\n",
      "     |      \n",
      "     |      D^2 is defined as\n",
      "     |      :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n",
      "     |      :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n",
      "     |      with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n",
      "     |      The mean :math:`\\bar{y}` is averaged by sample_weight.\n",
      "     |      Best possible score is 1.0 and it can be negative (because the model\n",
      "     |      can be arbitrarily worse).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True values of target.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          D^2 of self.predict(X) w.r.t. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "     |  RANSACRegressor(base_estimator=None, *, min_samples=None, residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, max_skips=inf, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, loss='absolute_loss', random_state=None)\n",
      "     |  \n",
      "     |  RANSAC (RANdom SAmple Consensus) algorithm.\n",
      "     |  \n",
      "     |  RANSAC is an iterative algorithm for the robust estimation of parameters\n",
      "     |  from a subset of inliers from the complete data set.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ransac_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, optional\n",
      "     |      Base estimator object which implements the following methods:\n",
      "     |  \n",
      "     |       * `fit(X, y)`: Fit model to given training data and target values.\n",
      "     |       * `score(X, y)`: Returns the mean accuracy on the given test data,\n",
      "     |         which is used for the stop criterion defined by `stop_score`.\n",
      "     |         Additionally, the score is used to decide which of two equally\n",
      "     |         large consensus sets is chosen as the better one.\n",
      "     |       * `predict(X)`: Returns predicted values using the linear model,\n",
      "     |         which is used to compute residual error using loss function.\n",
      "     |  \n",
      "     |      If `base_estimator` is None, then\n",
      "     |      ``base_estimator=sklearn.linear_model.LinearRegression()`` is used for\n",
      "     |      target values of dtype float.\n",
      "     |  \n",
      "     |      Note that the current implementation only supports regression\n",
      "     |      estimators.\n",
      "     |  \n",
      "     |  min_samples : int (>= 1) or float ([0, 1]), optional\n",
      "     |      Minimum number of samples chosen randomly from original data. Treated\n",
      "     |      as an absolute number of samples for `min_samples >= 1`, treated as a\n",
      "     |      relative number `ceil(min_samples * X.shape[0]`) for\n",
      "     |      `min_samples < 1`. This is typically chosen as the minimal number of\n",
      "     |      samples necessary to estimate the given `base_estimator`. By default a\n",
      "     |      ``sklearn.linear_model.LinearRegression()`` estimator is assumed and\n",
      "     |      `min_samples` is chosen as ``X.shape[1] + 1``.\n",
      "     |  \n",
      "     |  residual_threshold : float, optional\n",
      "     |      Maximum residual for a data sample to be classified as an inlier.\n",
      "     |      By default the threshold is chosen as the MAD (median absolute\n",
      "     |      deviation) of the target values `y`.\n",
      "     |  \n",
      "     |  is_data_valid : callable, optional\n",
      "     |      This function is called with the randomly selected data before the\n",
      "     |      model is fitted to it: `is_data_valid(X, y)`. If its return value is\n",
      "     |      False the current randomly chosen sub-sample is skipped.\n",
      "     |  \n",
      "     |  is_model_valid : callable, optional\n",
      "     |      This function is called with the estimated model and the randomly\n",
      "     |      selected data: `is_model_valid(model, X, y)`. If its return value is\n",
      "     |      False the current randomly chosen sub-sample is skipped.\n",
      "     |      Rejecting samples with this function is computationally costlier than\n",
      "     |      with `is_data_valid`. `is_model_valid` should therefore only be used if\n",
      "     |      the estimated model is needed for making the rejection decision.\n",
      "     |  \n",
      "     |  max_trials : int, optional\n",
      "     |      Maximum number of iterations for random sample selection.\n",
      "     |  \n",
      "     |  max_skips : int, optional\n",
      "     |      Maximum number of iterations that can be skipped due to finding zero\n",
      "     |      inliers or invalid data defined by ``is_data_valid`` or invalid models\n",
      "     |      defined by ``is_model_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  stop_n_inliers : int, optional\n",
      "     |      Stop iteration if at least this number of inliers are found.\n",
      "     |  \n",
      "     |  stop_score : float, optional\n",
      "     |      Stop iteration if score is greater equal than this threshold.\n",
      "     |  \n",
      "     |  stop_probability : float in range [0, 1], optional\n",
      "     |      RANSAC iteration stops if at least one outlier-free set of the training\n",
      "     |      data is sampled in RANSAC. This requires to generate at least N\n",
      "     |      samples (iterations)::\n",
      "     |  \n",
      "     |          N >= log(1 - probability) / log(1 - e**m)\n",
      "     |  \n",
      "     |      where the probability (confidence) is typically set to high value such\n",
      "     |      as 0.99 (the default) and e is the current fraction of inliers w.r.t.\n",
      "     |      the total number of samples.\n",
      "     |  \n",
      "     |  loss : string, callable, optional, default \"absolute_loss\"\n",
      "     |      String inputs, \"absolute_loss\" and \"squared_loss\" are supported which\n",
      "     |      find the absolute loss and squared loss per sample\n",
      "     |      respectively.\n",
      "     |  \n",
      "     |      If ``loss`` is a callable, then it should be a function that takes\n",
      "     |      two arrays as inputs, the true and predicted value and returns a 1-D\n",
      "     |      array with the i-th value of the array corresponding to the loss\n",
      "     |      on ``X[i]``.\n",
      "     |  \n",
      "     |      If the loss on a sample is greater than the ``residual_threshold``,\n",
      "     |      then this sample is classified as an outlier.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The generator used to initialize the centers.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : object\n",
      "     |      Best fitted model (copy of the `base_estimator` object).\n",
      "     |  \n",
      "     |  n_trials_ : int\n",
      "     |      Number of random selection trials until one of the stop criteria is\n",
      "     |      met. It is always ``<= max_trials``.\n",
      "     |  \n",
      "     |  inlier_mask_ : bool array of shape [n_samples]\n",
      "     |      Boolean mask of inliers classified as ``True``.\n",
      "     |  \n",
      "     |  n_skips_no_inliers_ : int\n",
      "     |      Number of iterations skipped due to finding zero inliers.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  n_skips_invalid_data_ : int\n",
      "     |      Number of iterations skipped due to invalid data defined by\n",
      "     |      ``is_data_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  n_skips_invalid_model_ : int\n",
      "     |      Number of iterations skipped due to an invalid model defined by\n",
      "     |      ``is_model_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import RANSACRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(\n",
      "     |  ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n",
      "     |  >>> reg = RANSACRegressor(random_state=0).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9885...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-31.9417...])\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] https://en.wikipedia.org/wiki/RANSAC\n",
      "     |  .. [2] https://www.sri.com/sites/default/files/publications/ransac-publication.pdf\n",
      "     |  .. [3] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RANSACRegressor\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, *, min_samples=None, residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, max_skips=inf, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, loss='absolute_loss', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit estimator using RANSAC algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample\n",
      "     |          raises error if sample_weight is passed and base_estimator\n",
      "     |          fit method does not support it.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError\n",
      "     |          If no valid consensus set could be found. This occurs if\n",
      "     |          `is_data_valid` and `is_model_valid` return False for all\n",
      "     |          `max_trials` randomly chosen sub-samples.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the estimated model.\n",
      "     |      \n",
      "     |      This is a wrapper for `estimator_.predict(X)`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y)\n",
      "     |      Returns the score of the prediction.\n",
      "     |      \n",
      "     |      This is a wrapper for `estimator_.score(X, y)`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array or sparse matrix of shape [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      z : float\n",
      "     |          Score of the prediction.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, _BaseRidge)\n",
      "     |  Ridge(alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
      "     |  \n",
      "     |  Linear least squares with l2 regularization.\n",
      "     |  \n",
      "     |  Minimizes the objective function::\n",
      "     |  \n",
      "     |  ||y - Xw||^2_2 + alpha * ||w||^2_2\n",
      "     |  \n",
      "     |  This model solves a regression model where the loss function is\n",
      "     |  the linear least squares function and regularization is given by\n",
      "     |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
      "     |  This estimator has built-in support for multi-variate regression\n",
      "     |  (i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : {float, ndarray of shape (n_targets,)}, default=1.0\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`sklearn.svm.LinearSVC`. If an array is passed, penalties are\n",
      "     |      assumed to be specific to the targets. Hence they must correspond in\n",
      "     |      number.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to fit the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. ``X`` and ``y`` are expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=None\n",
      "     |      Maximum number of iterations for conjugate gradient solver.\n",
      "     |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      "     |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Precision of the solution.\n",
      "     |  \n",
      "     |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\n",
      "     |      Solver to use in the computational routines:\n",
      "     |  \n",
      "     |      - 'auto' chooses the solver automatically based on the type of data.\n",
      "     |  \n",
      "     |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "     |        coefficients. More stable for singular matrices than 'cholesky'.\n",
      "     |  \n",
      "     |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "     |        obtain a closed-form solution.\n",
      "     |  \n",
      "     |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "     |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "     |        more appropriate than 'cholesky' for large-scale data\n",
      "     |        (possibility to set `tol` and `max_iter`).\n",
      "     |  \n",
      "     |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "     |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      "     |        procedure.\n",
      "     |  \n",
      "     |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "     |        its improved, unbiased version named SAGA. Both methods also use an\n",
      "     |        iterative procedure, and are often faster than other solvers when\n",
      "     |        both n_samples and n_features are large. Note that 'sag' and\n",
      "     |        'saga' fast convergence is only guaranteed on features with\n",
      "     |        approximately the same scale. You can preprocess the data with a\n",
      "     |        scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |      All last five solvers support both dense and sparse data. However, only\n",
      "     |      'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is\n",
      "     |      True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         `random_state` to support Stochastic Average Gradient.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  n_iter_ : None or ndarray of shape (n_targets,)\n",
      "     |      Actual number of iterations for each target. Available only for\n",
      "     |      sag and lsqr solvers. Other solvers will return None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  RidgeClassifier : Ridge classifier\n",
      "     |  RidgeCV : Ridge regression with built-in cross validation\n",
      "     |  :class:`sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n",
      "     |      combines ridge regression with the kernel trick\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import Ridge\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> y = rng.randn(n_samples)\n",
      "     |  >>> X = rng.randn(n_samples, n_features)\n",
      "     |  >>> clf = Ridge(alpha=1.0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  Ridge()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Ridge\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      _BaseRidge\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge regression model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, _BaseRidgeCV)\n",
      "     |  RidgeCV(alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False)\n",
      "     |  \n",
      "     |  Ridge regression with built-in cross-validation.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  By default, it performs Generalized Cross-Validation, which is a form of\n",
      "     |  efficient Leave-One-Out cross-validation.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alphas : ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n",
      "     |      Array of alpha values to try.\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`sklearn.svm.LinearSVC`.\n",
      "     |      If using generalized cross-validation, alphas must be positive.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  scoring : string, callable, default=None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``.\n",
      "     |      If None, the negative mean squared error if cv is 'auto' or None\n",
      "     |      (i.e. when using generalized cross-validation), and r2 score otherwise.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the efficient Leave-One-Out cross-validation\n",
      "     |        (also known as Generalized Cross-Validation).\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, if ``y`` is binary or multiclass,\n",
      "     |      :class:`sklearn.model_selection.StratifiedKFold` is used, else,\n",
      "     |      :class:`sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  gcv_mode : {'auto', 'svd', eigen'}, default='auto'\n",
      "     |      Flag indicating which strategy to use when performing\n",
      "     |      Generalized Cross-Validation. Options are::\n",
      "     |  \n",
      "     |          'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\n",
      "     |          'svd' : force use of singular value decomposition of X when X is\n",
      "     |              dense, eigenvalue decomposition of X^T.X when X is sparse.\n",
      "     |          'eigen' : force computation via eigendecomposition of X.X^T\n",
      "     |  \n",
      "     |      The 'auto' mode is the default and is intended to pick the cheaper\n",
      "     |      option of the two depending on the shape of the training data.\n",
      "     |  \n",
      "     |  store_cv_values : bool, default=False\n",
      "     |      Flag indicating if the cross-validation values corresponding to\n",
      "     |      each alpha should be stored in the ``cv_values_`` attribute (see\n",
      "     |      below). This flag is only compatible with ``cv=None`` (i.e. using\n",
      "     |      Generalized Cross-Validation).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_values_ : ndarray of shape (n_samples, n_alphas) or         shape (n_samples, n_targets, n_alphas), optional\n",
      "     |      Cross-validation values for each alpha (only available if         ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been         called, this attribute will contain the mean squared errors         (by default) or the values of the ``{loss,score}_func`` function         (if provided in the constructor).\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features) or (n_targets, n_features)\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      Estimated regularization parameter.\n",
      "     |  \n",
      "     |  best_score_ : float\n",
      "     |      Score of base estimator with best alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> from sklearn.linear_model import RidgeCV\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.5166...\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression\n",
      "     |  RidgeClassifier : Ridge classifier\n",
      "     |  RidgeClassifierCV : Ridge classifier with built-in cross validation\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      _BaseRidgeCV\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseRidgeCV:\n",
      "     |  \n",
      "     |  __init__(self, alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge regression model with cv.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training data. If using GCV, will be cast to float64\n",
      "     |          if necessary.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      When sample_weight is provided, the selected hyperparameter may depend\n",
      "     |      on whether we use generalized cross-validation (cv=None or cv='auto')\n",
      "     |      or another form of cross-validation, because only generalized\n",
      "     |      cross-validation takes the sample weights into account when computing\n",
      "     |      the validation score.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RidgeClassifier(sklearn.linear_model._base.LinearClassifierMixin, _BaseRidge)\n",
      "     |  RidgeClassifier(alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, class_weight=None, solver='auto', random_state=None)\n",
      "     |  \n",
      "     |  Classifier using Ridge regression.\n",
      "     |  \n",
      "     |  This classifier first converts the target values into ``{-1, 1}`` and\n",
      "     |  then treats the problem as a regression task (multi-output regression in\n",
      "     |  the multiclass case).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`sklearn.svm.LinearSVC`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set to false, no\n",
      "     |      intercept will be used in calculations (e.g. data is expected to be\n",
      "     |      already centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=None\n",
      "     |      Maximum number of iterations for conjugate gradient solver.\n",
      "     |      The default value is determined by scipy.sparse.linalg.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Precision of the solution.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\n",
      "     |      Solver to use in the computational routines:\n",
      "     |  \n",
      "     |      - 'auto' chooses the solver automatically based on the type of data.\n",
      "     |  \n",
      "     |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "     |        coefficients. More stable for singular matrices than 'cholesky'.\n",
      "     |  \n",
      "     |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "     |        obtain a closed-form solution.\n",
      "     |  \n",
      "     |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "     |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "     |        more appropriate than 'cholesky' for large-scale data\n",
      "     |        (possibility to set `tol` and `max_iter`).\n",
      "     |  \n",
      "     |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "     |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      "     |        procedure.\n",
      "     |  \n",
      "     |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "     |        its unbiased and more flexible version named SAGA. Both methods\n",
      "     |        use an iterative procedure, and are often faster than other solvers\n",
      "     |        when both n_samples and n_features are large. Note that 'sag' and\n",
      "     |        'saga' fast convergence is only guaranteed on features with\n",
      "     |        approximately the same scale. You can preprocess the data with a\n",
      "     |        scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |        .. versionadded:: 0.17\n",
      "     |           Stochastic Average Gradient descent solver.\n",
      "     |        .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      ``coef_`` is of shape (1, n_features) when the given problem is binary.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  n_iter_ : None or ndarray of shape (n_targets,)\n",
      "     |      Actual number of iterations for each target. Available only for\n",
      "     |      sag and lsqr solvers. Other solvers will return None.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression.\n",
      "     |  RidgeClassifierCV :  Ridge classifier with built-in cross validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For multi-class classification, n_class classifiers are trained in\n",
      "     |  a one-versus-all approach. Concretely, this is implemented by taking\n",
      "     |  advantage of the multi-variate response support in Ridge.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_breast_cancer\n",
      "     |  >>> from sklearn.linear_model import RidgeClassifier\n",
      "     |  >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "     |  >>> clf = RidgeClassifier().fit(X, y)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.9595...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseRidge\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, class_weight=None, solver='auto', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge classifier model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             *sample_weight* support to Classifier.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Instance of the estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RidgeClassifierCV(sklearn.linear_model._base.LinearClassifierMixin, _BaseRidgeCV)\n",
      "     |  RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize=False, scoring=None, cv=None, class_weight=None, store_cv_values=False)\n",
      "     |  \n",
      "     |  Ridge classifier with built-in cross-validation.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  By default, it performs Generalized Cross-Validation, which is a form of\n",
      "     |  efficient Leave-One-Out cross-validation. Currently, only the n_features >\n",
      "     |  n_samples case is handled efficiently.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alphas : ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n",
      "     |      Array of alpha values to try.\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`sklearn.svm.LinearSVC`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  normalize : bool, default=False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  scoring : string, callable, default=None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the efficient Leave-One-Out cross-validation\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  store_cv_values : bool, default=False\n",
      "     |      Flag indicating if the cross-validation values corresponding to\n",
      "     |      each alpha should be stored in the ``cv_values_`` attribute (see\n",
      "     |      below). This flag is only compatible with ``cv=None`` (i.e. using\n",
      "     |      Generalized Cross-Validation).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_values_ : ndarray of shape (n_samples, n_targets, n_alphas), optional\n",
      "     |      Cross-validation values for each alpha (if ``store_cv_values=True`` and\n",
      "     |      ``cv=None``). After ``fit()`` has been called, this attribute will\n",
      "     |      contain the mean squared errors (by default) or the values of the\n",
      "     |      ``{loss,score}_func`` function (if provided in the constructor). This\n",
      "     |      attribute exists only when ``store_cv_values`` is True.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_targets, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      ``coef_`` is of shape (1, n_features) when the given problem is binary.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      Estimated regularization parameter.\n",
      "     |  \n",
      "     |  best_score_ : float\n",
      "     |      Score of base estimator with best alpha.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_breast_cancer\n",
      "     |  >>> from sklearn.linear_model import RidgeClassifierCV\n",
      "     |  >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "     |  >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.9630...\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression\n",
      "     |  RidgeClassifier : Ridge classifier\n",
      "     |  RidgeCV : Ridge regression with built-in cross validation\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For multi-class classification, n_class classifiers are trained in\n",
      "     |  a one-versus-all approach. Concretely, this is implemented by taking\n",
      "     |  advantage of the multi-variate response support in Ridge.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeClassifierCV\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseRidgeCV\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, normalize=False, scoring=None, cv=None, class_weight=None, store_cv_values=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge classifier with cv.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features. When using GCV,\n",
      "     |          will be cast to float64 if necessary.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SGDClassifier(BaseSGDClassifier)\n",
      "     |  SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
      "     |  \n",
      "     |  This estimator implements regularized linear models with stochastic\n",
      "     |  gradient descent (SGD) learning: the gradient of the loss is estimated\n",
      "     |  each sample at a time and the model is updated along the way with a\n",
      "     |  decreasing strength schedule (aka learning rate). SGD allows minibatch\n",
      "     |  (online/out-of-core) learning via the `partial_fit` method.\n",
      "     |  For best results using the default learning rate schedule, the data should\n",
      "     |  have zero mean and unit variance.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense or sparse arrays\n",
      "     |  of floating point values for the features. The model it fits can be\n",
      "     |  controlled with the loss parameter; by default, it fits a linear support\n",
      "     |  vector machine (SVM).\n",
      "     |  \n",
      "     |  The regularizer is a penalty added to the loss function that shrinks model\n",
      "     |  parameters towards the zero vector using either the squared euclidean norm\n",
      "     |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "     |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      "     |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "     |  online feature selection.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : str, default='hinge'\n",
      "     |      The loss function to be used. Defaults to 'hinge', which gives a\n",
      "     |      linear SVM.\n",
      "     |  \n",
      "     |      The possible options are 'hinge', 'log', 'modified_huber',\n",
      "     |      'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |  \n",
      "     |      The 'log' loss gives logistic regression, a probabilistic classifier.\n",
      "     |      'modified_huber' is another smooth loss that brings tolerance to\n",
      "     |      outliers as well as probability estimates.\n",
      "     |      'squared_hinge' is like hinge but is quadratically penalized.\n",
      "     |      'perceptron' is the linear loss used by the perceptron algorithm.\n",
      "     |      The other losses are designed for regression but can be useful in\n",
      "     |      classification as well; see\n",
      "     |      :class:`~sklearn.linear_model.SGDRegressor` for a description.\n",
      "     |  \n",
      "     |      More details about the losses formulas can be found in the\n",
      "     |      :ref:`User Guide <sgd_mathematical_formulation>`.\n",
      "     |  \n",
      "     |  penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "     |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      "     |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      "     |      not achievable with 'l2'.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Constant that multiplies the regularization term. The higher the\n",
      "     |      value, the stronger the regularization.\n",
      "     |      Also used to compute the learning rate when set to `learning_rate` is\n",
      "     |      set to 'optimal'.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.15\n",
      "     |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "     |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "     |      Only used if `penalty` is 'elasticnet'.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, training will stop\n",
      "     |      when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      "     |      epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  epsilon : float, default=0.1\n",
      "     |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |      For 'huber', determines the threshold at which it becomes less\n",
      "     |      important to get the prediction exactly right.\n",
      "     |      For epsilon-insensitive, any differences between the current prediction\n",
      "     |      and the correct label are ignored if they are less than this threshold.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  learning_rate : str, default='optimal'\n",
      "     |      The learning rate schedule:\n",
      "     |  \n",
      "     |      - 'constant': `eta = eta0`\n",
      "     |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      "     |        where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      "     |      - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      "     |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      "     |        training loss by tol or fail to increase validation score by tol if\n",
      "     |        early_stopping is True, the current learning rate is divided by 5.\n",
      "     |  \n",
      "     |          .. versionadded:: 0.20\n",
      "     |              Added 'adaptive' option\n",
      "     |  \n",
      "     |  eta0 : double, default=0.0\n",
      "     |      The initial learning rate for the 'constant', 'invscaling' or\n",
      "     |      'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n",
      "     |      the default schedule 'optimal'.\n",
      "     |  \n",
      "     |  power_t : double, default=0.5\n",
      "     |      The exponent for inverse scaling learning rate [default 0.5].\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a stratified fraction of training data as validation and terminate\n",
      "     |      training when validation score returned by the `score` method is not\n",
      "     |      improving by at least tol for n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'early_stopping' option\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if `early_stopping` is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'validation_fraction' option\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'n_iter_no_change' option\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |      If a dynamic learning rate is used, the learning rate is adapted\n",
      "     |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      "     |      this counter, while ``partial_fit`` will result in increasing the\n",
      "     |      existing counter.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights accross all\n",
      "     |      updates and stores the result in the ``coef_`` attribute. If set to\n",
      "     |      an int greater than 1, averaging will begin once the total number of\n",
      "     |      samples seen reaches `average`. So ``average=10`` will begin\n",
      "     |      averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations before reaching the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  loss_function_ : concrete ``LossFunction``\n",
      "     |  \n",
      "     |  classes_ : array of shape (n_classes,)\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.svm.LinearSVC: Linear support vector classification.\n",
      "     |  LogisticRegression: Logistic regression.\n",
      "     |  Perceptron: Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n",
      "     |      ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n",
      "     |      penalty=None)``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import SGDClassifier\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> Y = np.array([1, 1, 2, 2])\n",
      "     |  >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      "     |  >>> clf = make_pipeline(StandardScaler(),\n",
      "     |  ...                     SGDClassifier(max_iter=1000, tol=1e-3))\n",
      "     |  >>> clf.fit(X, Y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "     |                  ('sgdclassifier', SGDClassifier())])\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDClassifier\n",
      "     |      BaseSGDClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  predict_log_proba\n",
      "     |      Log of probability estimates.\n",
      "     |      \n",
      "     |      This method is only available for log loss and modified Huber loss.\n",
      "     |      \n",
      "     |      When loss=\"modified_huber\", probability estimates may be hard zeros\n",
      "     |      and ones, so taking the logarithm is not possible.\n",
      "     |      \n",
      "     |      See ``predict_proba`` for details.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data for prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in\n",
      "     |          `self.classes_`.\n",
      "     |  \n",
      "     |  predict_proba\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      This method is only available for log loss and modified Huber loss.\n",
      "     |      \n",
      "     |      Multiclass probability estimates are derived from binary (one-vs.-rest)\n",
      "     |      estimates by simple normalization, as recommended by Zadrozny and\n",
      "     |      Elkan.\n",
      "     |      \n",
      "     |      Binary probability estimates for loss=\"modified_huber\" are given by\n",
      "     |      (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\n",
      "     |      it is necessary to perform proper probability calibration by wrapping\n",
      "     |      the classifier with\n",
      "     |      :class:`sklearn.calibration.CalibratedClassifierCV` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Input data for prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ndarray of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in `self.classes_`.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      Zadrozny and Elkan, \"Transforming classifier scores into multiclass\n",
      "     |      probability estimates\", SIGKDD'02,\n",
      "     |      http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf\n",
      "     |      \n",
      "     |      The justification for the formula in the loss=\"modified_huber\"\n",
      "     |      case is in the appendix B in:\n",
      "     |      http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGDClassifier:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_classes, n_features), default=None\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (n_classes,), default=None\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self :\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Perform one epoch of stochastic gradient descent on given samples.\n",
      "     |      \n",
      "     |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      "     |      guaranteed that a minimum of the cost function is reached after calling\n",
      "     |      it once. Matters such as objective convergence and early stopping\n",
      "     |      should be handled by the user.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Subset of the target values.\n",
      "     |      \n",
      "     |      classes : ndarray of shape (n_classes,), default=None\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self :\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, **kwargs)\n",
      "     |      Set and validate the parameters of estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseSGD:\n",
      "     |  \n",
      "     |  average_coef_\n",
      "     |  \n",
      "     |  average_intercept_\n",
      "     |  \n",
      "     |  standard_coef_\n",
      "     |  \n",
      "     |  standard_intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "    \n",
      "    class SGDRegressor(BaseSGDRegressor)\n",
      "     |  SGDRegressor(loss='squared_loss', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Linear model fitted by minimizing a regularized empirical loss with SGD\n",
      "     |  \n",
      "     |  SGD stands for Stochastic Gradient Descent: the gradient of the loss is\n",
      "     |  estimated each sample at a time and the model is updated along the way with\n",
      "     |  a decreasing strength schedule (aka learning rate).\n",
      "     |  \n",
      "     |  The regularizer is a penalty added to the loss function that shrinks model\n",
      "     |  parameters towards the zero vector using either the squared euclidean norm\n",
      "     |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "     |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      "     |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "     |  online feature selection.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense numpy arrays of\n",
      "     |  floating point values for the features.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : str, default='squared_loss'\n",
      "     |      The loss function to be used. The possible values are 'squared_loss',\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\n",
      "     |  \n",
      "     |      The 'squared_loss' refers to the ordinary least squares fit.\n",
      "     |      'huber' modifies 'squared_loss' to focus less on getting outliers\n",
      "     |      correct by switching from squared to linear loss past a distance of\n",
      "     |      epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\n",
      "     |      linear past that; this is the loss function used in SVR.\n",
      "     |      'squared_epsilon_insensitive' is the same but becomes squared loss past\n",
      "     |      a tolerance of epsilon.\n",
      "     |  \n",
      "     |      More details about the losses formulas can be found in the\n",
      "     |      :ref:`User Guide <sgd_mathematical_formulation>`.\n",
      "     |  \n",
      "     |  penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "     |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      "     |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      "     |      not achievable with 'l2'.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Constant that multiplies the regularization term. The higher the\n",
      "     |      value, the stronger the regularization.\n",
      "     |      Also used to compute the learning rate when set to `learning_rate` is\n",
      "     |      set to 'optimal'.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.15\n",
      "     |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "     |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "     |      Only used if `penalty` is 'elasticnet'.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, training will stop\n",
      "     |      when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      "     |      epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  epsilon : float, default=0.1\n",
      "     |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |      For 'huber', determines the threshold at which it becomes less\n",
      "     |      important to get the prediction exactly right.\n",
      "     |      For epsilon-insensitive, any differences between the current prediction\n",
      "     |      and the correct label are ignored if they are less than this threshold.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  learning_rate : string, default='invscaling'\n",
      "     |      The learning rate schedule:\n",
      "     |  \n",
      "     |      - 'constant': `eta = eta0`\n",
      "     |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      "     |        where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      "     |      - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      "     |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      "     |        training loss by tol or fail to increase validation score by tol if\n",
      "     |        early_stopping is True, the current learning rate is divided by 5.\n",
      "     |  \n",
      "     |          .. versionadded:: 0.20\n",
      "     |              Added 'adaptive' option\n",
      "     |  \n",
      "     |  eta0 : double, default=0.01\n",
      "     |      The initial learning rate for the 'constant', 'invscaling' or\n",
      "     |      'adaptive' schedules. The default value is 0.01.\n",
      "     |  \n",
      "     |  power_t : double, default=0.25\n",
      "     |      The exponent for inverse scaling learning rate.\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a fraction of training data as validation and terminate\n",
      "     |      training when validation score returned by the `score` method is not\n",
      "     |      improving by at least `tol` for `n_iter_no_change` consecutive\n",
      "     |      epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'early_stopping' option\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if `early_stopping` is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'validation_fraction' option\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'n_iter_no_change' option\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |      If a dynamic learning rate is used, the learning rate is adapted\n",
      "     |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      "     |      this counter, while ``partial_fit``  will result in increasing the\n",
      "     |      existing counter.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights accross all\n",
      "     |      updates and stores the result in the ``coef_`` attribute. If set to\n",
      "     |      an int greater than 1, averaging will begin once the total number of\n",
      "     |      samples seen reaches `average`. So ``average=10`` will begin\n",
      "     |      averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,)\n",
      "     |      The intercept term.\n",
      "     |  \n",
      "     |  average_coef_ : ndarray of shape (n_features,)\n",
      "     |      Averaged weights assigned to the features. Only available\n",
      "     |      if ``average=True``.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.23\n",
      "     |          Attribute ``average_coef_`` was deprecated\n",
      "     |          in version 0.23 and will be removed in 0.25.\n",
      "     |  \n",
      "     |  average_intercept_ : ndarray of shape (1,)\n",
      "     |      The averaged intercept term. Only available if ``average=True``.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.23\n",
      "     |          Attribute ``average_intercept_`` was deprecated\n",
      "     |          in version 0.23 and will be removed in 0.25.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations before reaching the stopping criterion.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples)``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import SGDRegressor\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> y = rng.randn(n_samples)\n",
      "     |  >>> X = rng.randn(n_samples, n_features)\n",
      "     |  >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      "     |  >>> reg = make_pipeline(StandardScaler(),\n",
      "     |  ...                     SGDRegressor(max_iter=1000, tol=1e-3))\n",
      "     |  >>> reg.fit(X, y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "     |                  ('sgdregressor', SGDRegressor())])\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  Ridge, ElasticNet, Lasso, sklearn.svm.SVR\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDRegressor\n",
      "     |      BaseSGDRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='squared_loss', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGDRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features,), default=None\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (1,), default=None\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, sample_weight=None)\n",
      "     |      Perform one epoch of stochastic gradient descent on given samples.\n",
      "     |      \n",
      "     |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      "     |      guaranteed that a minimum of the cost function is reached after calling\n",
      "     |      it once. Matters such as objective convergence and early stopping\n",
      "     |      should be handled by the user.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of training data\n",
      "     |      \n",
      "     |      y : numpy array of shape (n_samples,)\n",
      "     |          Subset of target values\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ndarray of shape (n_samples,)\n",
      "     |         Predicted target values per element in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseSGDRegressor:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, **kwargs)\n",
      "     |      Set and validate the parameters of estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseSGD:\n",
      "     |  \n",
      "     |  average_coef_\n",
      "     |  \n",
      "     |  average_intercept_\n",
      "     |  \n",
      "     |  standard_coef_\n",
      "     |  \n",
      "     |  standard_intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "    \n",
      "    class SquaredLoss(Regression)\n",
      "     |  Squared loss traditional used in linear regression.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SquaredLoss\n",
      "     |      Regression\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Regression:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class TheilSenRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  TheilSenRegressor(*, fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=None, verbose=False)\n",
      "     |  \n",
      "     |  Theil-Sen Estimator: robust multivariate regression model.\n",
      "     |  \n",
      "     |  The algorithm calculates least square solutions on subsets with size\n",
      "     |  n_subsamples of the samples in X. Any value of n_subsamples between the\n",
      "     |  number of features and samples leads to an estimator with a compromise\n",
      "     |  between robustness and efficiency. Since the number of least square\n",
      "     |  solutions is \"n_samples choose n_subsamples\", it can be extremely large\n",
      "     |  and can therefore be limited with max_subpopulation. If this limit is\n",
      "     |  reached, the subsets are chosen randomly. In a final step, the spatial\n",
      "     |  median (or L1 median) is calculated of all least square solutions.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <theil_sen_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : boolean, optional, default True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_subpopulation : int, optional, default 1e4\n",
      "     |      Instead of computing with a set of cardinality 'n choose k', where n is\n",
      "     |      the number of samples and k is the number of subsamples (at least\n",
      "     |      number of features), consider only a stochastic subpopulation of a\n",
      "     |      given maximal size if 'n choose k' is larger than max_subpopulation.\n",
      "     |      For other than small problem sizes this parameter will determine\n",
      "     |      memory usage and runtime if n_subsamples is not changed.\n",
      "     |  \n",
      "     |  n_subsamples : int, optional, default None\n",
      "     |      Number of samples to calculate the parameters. This is at least the\n",
      "     |      number of features (plus 1 if fit_intercept=True) and the number of\n",
      "     |      samples as a maximum. A lower number leads to a higher breakdown\n",
      "     |      point and a low efficiency while a high number leads to a low\n",
      "     |      breakdown point and a high efficiency. If None, take the\n",
      "     |      minimum number of subsamples leading to maximal robustness.\n",
      "     |      If n_subsamples is set to n_samples, Theil-Sen is identical to least\n",
      "     |      squares.\n",
      "     |  \n",
      "     |  max_iter : int, optional, default 300\n",
      "     |      Maximum number of iterations for the calculation of spatial median.\n",
      "     |  \n",
      "     |  tol : float, optional, default 1.e-3\n",
      "     |      Tolerance when calculating spatial median.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      A random number generator instance to define the state of the random\n",
      "     |      permutations generator. Pass an int for reproducible output across\n",
      "     |      multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`\n",
      "     |  \n",
      "     |  n_jobs : int or None, optional (default=None)\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : boolean, optional, default False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = (n_features)\n",
      "     |      Coefficients of the regression model (median of distribution).\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Estimated intercept of regression model.\n",
      "     |  \n",
      "     |  breakdown_ : float\n",
      "     |      Approximated breakdown point.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations needed for the spatial median.\n",
      "     |  \n",
      "     |  n_subpopulation_ : int\n",
      "     |      Number of combinations taken into account from 'n choose k', where n is\n",
      "     |      the number of samples and k is the number of subsamples.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import TheilSenRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(\n",
      "     |  ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n",
      "     |  >>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9884...\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-31.5871...])\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\n",
      "     |    Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\n",
      "     |    http://home.olemiss.edu/~xdang/papers/MTSE.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TheilSenRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=None, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training data\n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix or a list of generic objects instead,\n",
      "     |          shape = (n_samples, n_samples_fitted),\n",
      "     |          where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class TweedieRegressor(GeneralizedLinearRegressor)\n",
      "     |  TweedieRegressor(*, power=0.0, alpha=1.0, fit_intercept=True, link='auto', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |  \n",
      "     |  Generalized Linear Model with a Tweedie distribution.\n",
      "     |  \n",
      "     |  This estimator can be used to model different GLMs depending on the\n",
      "     |  ``power`` parameter, which determines the underlying distribution.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <Generalized_linear_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  power : float, default=0\n",
      "     |          The power determines the underlying target distribution according\n",
      "     |          to the following table:\n",
      "     |  \n",
      "     |          +-------+------------------------+\n",
      "     |          | Power | Distribution           |\n",
      "     |          +=======+========================+\n",
      "     |          | 0     | Normal                 |\n",
      "     |          +-------+------------------------+\n",
      "     |          | 1     | Poisson                |\n",
      "     |          +-------+------------------------+\n",
      "     |          | (1,2) | Compound Poisson Gamma |\n",
      "     |          +-------+------------------------+\n",
      "     |          | 2     | Gamma                  |\n",
      "     |          +-------+------------------------+\n",
      "     |          | 3     | Inverse Gaussian       |\n",
      "     |          +-------+------------------------+\n",
      "     |  \n",
      "     |          For ``0 < power < 1``, no distribution exists.\n",
      "     |  \n",
      "     |  alpha : float, default=1\n",
      "     |      Constant that multiplies the penalty term and thus determines the\n",
      "     |      regularization strength. ``alpha = 0`` is equivalent to unpenalized\n",
      "     |      GLMs. In this case, the design matrix `X` must have full column rank\n",
      "     |      (no collinearities).\n",
      "     |  \n",
      "     |  link : {'auto', 'identity', 'log'}, default='auto'\n",
      "     |      The link function of the GLM, i.e. mapping from linear predictor\n",
      "     |      `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets\n",
      "     |      the link depending on the chosen family as follows:\n",
      "     |  \n",
      "     |      - 'identity' for Normal distribution\n",
      "     |      - 'log' for Poisson,  Gamma and Inverse Gaussian distributions\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the linear predictor (X @ coef + intercept).\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximal number of iterations for the solver.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Stopping criterion. For the lbfgs solver,\n",
      "     |      the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n",
      "     |      where ``g_j`` is the j-th component of the gradient (derivative) of\n",
      "     |      the objective function.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If set to ``True``, reuse the solution of the previous call to ``fit``\n",
      "     |      as initialization for ``coef_`` and ``intercept_`` .\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the lbfgs solver set verbose to any positive number for verbosity.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the linear predictor (`X @ coef_ +\n",
      "     |      intercept_`) in the GLM.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Intercept (a.k.a. bias) added to linear predictor.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Actual number of iterations used in the solver.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TweedieRegressor\n",
      "     |      GeneralizedLinearRegressor\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, power=0.0, alpha=1.0, fit_intercept=True, link='auto', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  family\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from GeneralizedLinearRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit a Generalized Linear Model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using GLM with feature matrix X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array of shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Compute D^2, the percentage of deviance explained.\n",
      "     |      \n",
      "     |      D^2 is a generalization of the coefficient of determination R^2.\n",
      "     |      R^2 uses squared error and D^2 deviance. Note that those two are equal\n",
      "     |      for ``family='normal'``.\n",
      "     |      \n",
      "     |      D^2 is defined as\n",
      "     |      :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n",
      "     |      :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n",
      "     |      with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n",
      "     |      The mean :math:`\\bar{y}` is averaged by sample_weight.\n",
      "     |      Best possible score is 1.0 and it can be negative (because the model\n",
      "     |      can be arbitrarily worse).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True values of target.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          D^2 of self.predict(X) w.r.t. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "        Compute elastic net path with coordinate descent.\n",
      "        \n",
      "        The elastic net optimization function varies for mono and multi-outputs.\n",
      "        \n",
      "        For mono-output tasks it is::\n",
      "        \n",
      "            1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "            + alpha * l1_ratio * ||w||_1\n",
      "            + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "        \n",
      "        For multi-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "            + alpha * l1_ratio * ||W||_21\n",
      "            + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "        \n",
      "        Where::\n",
      "        \n",
      "            ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "        \n",
      "        i.e. the sum of norm of each row.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <elastic_net>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "            unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "            can be sparse.\n",
      "        \n",
      "        y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "            Target values.\n",
      "        \n",
      "        l1_ratio : float, default=0.5\n",
      "            Number between 0 and 1 passed to elastic net (scaling between\n",
      "            l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "        \n",
      "        eps : float, default=1e-3\n",
      "            Length of the path. ``eps=1e-3`` means that\n",
      "            ``alpha_min / alpha_max = 1e-3``.\n",
      "        \n",
      "        n_alphas : int, default=100\n",
      "            Number of alphas along the regularization path.\n",
      "        \n",
      "        alphas : ndarray, default=None\n",
      "            List of alphas where to compute the models.\n",
      "            If None alphas are set automatically.\n",
      "        \n",
      "        precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "            Whether to use a precomputed Gram matrix to speed up\n",
      "            calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "            matrix can also be passed as argument.\n",
      "        \n",
      "        Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``True``, X will be copied; else, it may be overwritten.\n",
      "        \n",
      "        coef_init : ndarray of shape (n_features, ), default=None\n",
      "            The initial values of the coefficients.\n",
      "        \n",
      "        verbose : bool or int, default=False\n",
      "            Amount of verbosity.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations or not.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            If set to True, forces coefficients to be positive.\n",
      "            (Only allowed when ``y.ndim == 1``).\n",
      "        \n",
      "        check_input : bool, default=True\n",
      "            Skip input validation checks, including the Gram matrix when provided\n",
      "            assuming there are handled by the caller when check_input=False.\n",
      "        \n",
      "        **params : kwargs\n",
      "            Keyword arguments passed to the coordinate descent solver.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : ndarray of shape (n_alphas,)\n",
      "            The alphas along the path where models are computed.\n",
      "        \n",
      "        coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        dual_gaps : ndarray of shape (n_alphas,)\n",
      "            The dual gaps at the end of the optimization for each alpha.\n",
      "        \n",
      "        n_iters : list of int\n",
      "            The number of iterations taken by the coordinate descent optimizer to\n",
      "            reach the specified tolerance for each alpha.\n",
      "            (Is returned when ``return_n_iter`` is set to True).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        MultiTaskElasticNet\n",
      "        MultiTaskElasticNetCV\n",
      "        ElasticNet\n",
      "        ElasticNetCV\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see\n",
      "        :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "        <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "    \n",
      "    lars_path(X, y, Xy=None, *, Gram=None, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.220446049250313e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)\n",
      "        Compute Least Angle Regression or Lasso path using LARS algorithm [1]\n",
      "        \n",
      "        The optimization objective for the case method='lasso' is::\n",
      "        \n",
      "        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        in the case of method='lars', the objective function is only known in\n",
      "        the form of an implicit equation (see discussion in [1])\n",
      "        \n",
      "        Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : None or array-like of shape (n_samples, n_features)\n",
      "            Input data. Note that if X is None then the Gram matrix must be\n",
      "            specified, i.e., cannot be None or False.\n",
      "        \n",
      "        y : None or array-like of shape (n_samples,)\n",
      "            Input targets.\n",
      "        \n",
      "        Xy : array-like of shape (n_samples,) or (n_samples, n_targets),             default=None\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        Gram : None, 'auto', array-like of shape (n_features, n_features),             default=None\n",
      "            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram\n",
      "            matrix is precomputed from the given X, if there are more samples\n",
      "            than features.\n",
      "        \n",
      "        max_iter : int, default=500\n",
      "            Maximum number of iterations to perform, set to infinity for no limit.\n",
      "        \n",
      "        alpha_min : float, default=0\n",
      "            Minimum correlation along the path. It corresponds to the\n",
      "            regularization parameter alpha parameter in the Lasso.\n",
      "        \n",
      "        method : {'lar', 'lasso'}, default='lar'\n",
      "            Specifies the returned model. Select ``'lar'`` for Least Angle\n",
      "            Regression, ``'lasso'`` for the Lasso.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``False``, ``X`` is overwritten.\n",
      "        \n",
      "        eps : float, optional\n",
      "            The machine-precision regularization in the computation of the\n",
      "            Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "            systems. By default, ``np.finfo(np.float).eps`` is used.\n",
      "        \n",
      "        copy_Gram : bool, default=True\n",
      "            If ``False``, ``Gram`` is overwritten.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls output verbosity.\n",
      "        \n",
      "        return_path : bool, default=True\n",
      "            If ``return_path==True`` returns the entire path, else returns only the\n",
      "            last point of the path.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            Restrict coefficients to be >= 0.\n",
      "            This option is only allowed with method 'lasso'. Note that the model\n",
      "            coefficients will not converge to the ordinary-least-squares solution\n",
      "            for small values of alpha. Only coefficients up to the smallest alpha\n",
      "            value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by\n",
      "            the stepwise Lars-Lasso algorithm are typically in congruence with the\n",
      "            solution of the coordinate descent lasso_path function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : array-like of shape (n_alphas + 1,)\n",
      "            Maximum of covariances (in absolute value) at each iteration.\n",
      "            ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "            number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "            is smaller.\n",
      "        \n",
      "        active : array-like of shape (n_alphas,)\n",
      "            Indices of active variables at the end of the path.\n",
      "        \n",
      "        coefs : array-like of shape (n_features, n_alphas + 1)\n",
      "            Coefficients along the path\n",
      "        \n",
      "        n_iter : int\n",
      "            Number of iterations run. Returned only if return_n_iter is set\n",
      "            to True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        lars_path_gram\n",
      "        lasso_path\n",
      "        lasso_path_gram\n",
      "        LassoLars\n",
      "        Lars\n",
      "        LassoLarsCV\n",
      "        LarsCV\n",
      "        sklearn.decomposition.sparse_encode\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Least Angle Regression\", Efron et al.\n",
      "               http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Least-angle regression\n",
      "               <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n",
      "        \n",
      "        .. [3] `Wikipedia entry on the Lasso\n",
      "               <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n",
      "    \n",
      "    lars_path_gram(Xy, Gram, *, n_samples, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.220446049250313e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)\n",
      "        lars_path in the sufficient stats mode [1]\n",
      "        \n",
      "        The optimization objective for the case method='lasso' is::\n",
      "        \n",
      "        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        in the case of method='lars', the objective function is only known in\n",
      "        the form of an implicit equation (see discussion in [1])\n",
      "        \n",
      "        Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Xy : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "            Xy = np.dot(X.T, y).\n",
      "        \n",
      "        Gram : array-like of shape (n_features, n_features)\n",
      "            Gram = np.dot(X.T * X).\n",
      "        \n",
      "        n_samples : int or float\n",
      "            Equivalent size of sample.\n",
      "        \n",
      "        max_iter : int, default=500\n",
      "            Maximum number of iterations to perform, set to infinity for no limit.\n",
      "        \n",
      "        alpha_min : float, default=0\n",
      "            Minimum correlation along the path. It corresponds to the\n",
      "            regularization parameter alpha parameter in the Lasso.\n",
      "        \n",
      "        method : {'lar', 'lasso'}, default='lar'\n",
      "            Specifies the returned model. Select ``'lar'`` for Least Angle\n",
      "            Regression, ``'lasso'`` for the Lasso.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``False``, ``X`` is overwritten.\n",
      "        \n",
      "        eps : float, optional\n",
      "            The machine-precision regularization in the computation of the\n",
      "            Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "            systems. By default, ``np.finfo(np.float).eps`` is used.\n",
      "        \n",
      "        copy_Gram : bool, default=True\n",
      "            If ``False``, ``Gram`` is overwritten.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls output verbosity.\n",
      "        \n",
      "        return_path : bool, default=True\n",
      "            If ``return_path==True`` returns the entire path, else returns only the\n",
      "            last point of the path.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            Restrict coefficients to be >= 0.\n",
      "            This option is only allowed with method 'lasso'. Note that the model\n",
      "            coefficients will not converge to the ordinary-least-squares solution\n",
      "            for small values of alpha. Only coefficients up to the smallest alpha\n",
      "            value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by\n",
      "            the stepwise Lars-Lasso algorithm are typically in congruence with the\n",
      "            solution of the coordinate descent lasso_path function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : array-like of shape (n_alphas + 1,)\n",
      "            Maximum of covariances (in absolute value) at each iteration.\n",
      "            ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "            number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "            is smaller.\n",
      "        \n",
      "        active : array-like of shape (n_alphas,)\n",
      "            Indices of active variables at the end of the path.\n",
      "        \n",
      "        coefs : array-like of shape (n_features, n_alphas + 1)\n",
      "            Coefficients along the path\n",
      "        \n",
      "        n_iter : int\n",
      "            Number of iterations run. Returned only if return_n_iter is set\n",
      "            to True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        lars_path\n",
      "        lasso_path\n",
      "        lasso_path_gram\n",
      "        LassoLars\n",
      "        Lars\n",
      "        LassoLarsCV\n",
      "        LarsCV\n",
      "        sklearn.decomposition.sparse_encode\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Least Angle Regression\", Efron et al.\n",
      "               http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Least-angle regression\n",
      "               <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n",
      "        \n",
      "        .. [3] `Wikipedia entry on the Lasso\n",
      "               <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n",
      "    \n",
      "    lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "        Compute Lasso path with coordinate descent\n",
      "        \n",
      "        The Lasso optimization function varies for mono and multi-outputs.\n",
      "        \n",
      "        For mono-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        For multi-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "        \n",
      "        Where::\n",
      "        \n",
      "            ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "        \n",
      "        i.e. the sum of norm of each row.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <lasso>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "            unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "            can be sparse.\n",
      "        \n",
      "        y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_outputs)\n",
      "            Target values\n",
      "        \n",
      "        eps : float, default=1e-3\n",
      "            Length of the path. ``eps=1e-3`` means that\n",
      "            ``alpha_min / alpha_max = 1e-3``\n",
      "        \n",
      "        n_alphas : int, default=100\n",
      "            Number of alphas along the regularization path\n",
      "        \n",
      "        alphas : ndarray, default=None\n",
      "            List of alphas where to compute the models.\n",
      "            If ``None`` alphas are set automatically\n",
      "        \n",
      "        precompute : 'auto', bool or array-like of shape (n_features, n_features),                 default='auto'\n",
      "            Whether to use a precomputed Gram matrix to speed up\n",
      "            calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "            matrix can also be passed as argument.\n",
      "        \n",
      "        Xy : array-like of shape (n_features,) or (n_features, n_outputs),         default=None\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``True``, X will be copied; else, it may be overwritten.\n",
      "        \n",
      "        coef_init : ndarray of shape (n_features, ), default=None\n",
      "            The initial values of the coefficients.\n",
      "        \n",
      "        verbose : bool or int, default=False\n",
      "            Amount of verbosity.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            whether to return the number of iterations or not.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            If set to True, forces coefficients to be positive.\n",
      "            (Only allowed when ``y.ndim == 1``).\n",
      "        \n",
      "        **params : kwargs\n",
      "            keyword arguments passed to the coordinate descent solver.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : ndarray of shape (n_alphas,)\n",
      "            The alphas along the path where models are computed.\n",
      "        \n",
      "        coefs : ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        dual_gaps : ndarray of shape (n_alphas,)\n",
      "            The dual gaps at the end of the optimization for each alpha.\n",
      "        \n",
      "        n_iters : list of int\n",
      "            The number of iterations taken by the coordinate descent optimizer to\n",
      "            reach the specified tolerance for each alpha.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see\n",
      "        :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "        <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "        \n",
      "        To avoid unnecessary memory duplication the X argument of the fit method\n",
      "        should be directly passed as a Fortran-contiguous numpy array.\n",
      "        \n",
      "        Note that in certain cases, the Lars solver may be significantly\n",
      "        faster to implement this functionality. In particular, linear\n",
      "        interpolation can be used to retrieve model coefficients between the\n",
      "        values output by lars_path\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Comparing lasso_path and lars_path with interpolation:\n",
      "        \n",
      "        >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "        >>> y = np.array([1, 2, 3.1])\n",
      "        >>> # Use lasso_path to compute a coefficient path\n",
      "        >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "        >>> print(coef_path)\n",
      "        [[0.         0.         0.46874778]\n",
      "         [0.2159048  0.4425765  0.23689075]]\n",
      "        \n",
      "        >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "        >>> # same path\n",
      "        >>> from sklearn.linear_model import lars_path\n",
      "        >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "        >>> from scipy import interpolate\n",
      "        >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "        ...                                             coef_path_lars[:, ::-1])\n",
      "        >>> print(coef_path_continuous([5., 1., .5]))\n",
      "        [[0.         0.         0.46915237]\n",
      "         [0.2159048  0.4425765  0.23668876]]\n",
      "        \n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        lars_path\n",
      "        Lasso\n",
      "        LassoLars\n",
      "        LassoCV\n",
      "        LassoLarsCV\n",
      "        sklearn.decomposition.sparse_encode\n",
      "    \n",
      "    orthogonal_mp(X, y, *, n_nonzero_coefs=None, tol=None, precompute=False, copy_X=True, return_path=False, return_n_iter=False)\n",
      "        Orthogonal Matching Pursuit (OMP)\n",
      "        \n",
      "        Solves n_targets Orthogonal Matching Pursuit problems.\n",
      "        An instance of the problem has the form:\n",
      "        \n",
      "        When parametrized by the number of non-zero coefficients using\n",
      "        `n_nonzero_coefs`:\n",
      "        argmin ||y - X\\gamma||^2 subject to ||\\gamma||_0 <= n_{nonzero coefs}\n",
      "        \n",
      "        When parametrized by error using the parameter `tol`:\n",
      "        argmin ||\\gamma||_0 subject to ||y - X\\gamma||^2 <= tol\n",
      "        \n",
      "        Read more in the :ref:`User Guide <omp>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array, shape (n_samples, n_features)\n",
      "            Input data. Columns are assumed to have unit norm.\n",
      "        \n",
      "        y : array, shape (n_samples,) or (n_samples, n_targets)\n",
      "            Input targets\n",
      "        \n",
      "        n_nonzero_coefs : int\n",
      "            Desired number of non-zero entries in the solution. If None (by\n",
      "            default) this value is set to 10% of n_features.\n",
      "        \n",
      "        tol : float\n",
      "            Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "        \n",
      "        precompute : {True, False, 'auto'},\n",
      "            Whether to perform precomputations. Improves performance when n_targets\n",
      "            or n_samples is very large.\n",
      "        \n",
      "        copy_X : bool, optional\n",
      "            Whether the design matrix X must be copied by the algorithm. A false\n",
      "            value is only helpful if X is already Fortran-ordered, otherwise a\n",
      "            copy is made anyway.\n",
      "        \n",
      "        return_path : bool, optional. Default: False\n",
      "            Whether to return every value of the nonzero coefficients along the\n",
      "            forward path. Useful for cross-validation.\n",
      "        \n",
      "        return_n_iter : bool, optional default False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : array, shape (n_features,) or (n_features, n_targets)\n",
      "            Coefficients of the OMP solution. If `return_path=True`, this contains\n",
      "            the whole coefficient path. In this case its shape is\n",
      "            (n_features, n_features) or (n_features, n_targets, n_features) and\n",
      "            iterating over the last axis yields coefficients in increasing order\n",
      "            of active features.\n",
      "        \n",
      "        n_iters : array-like or int\n",
      "            Number of active features across every target. Returned only if\n",
      "            `return_n_iter` is set to True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        OrthogonalMatchingPursuit\n",
      "        orthogonal_mp_gram\n",
      "        lars_path\n",
      "        decomposition.sparse_encode\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang,\n",
      "        Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "        Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "        (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "        \n",
      "        This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "        M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "        Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "        https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "    \n",
      "    orthogonal_mp_gram(Gram, Xy, *, n_nonzero_coefs=None, tol=None, norms_squared=None, copy_Gram=True, copy_Xy=True, return_path=False, return_n_iter=False)\n",
      "        Gram Orthogonal Matching Pursuit (OMP)\n",
      "        \n",
      "        Solves n_targets Orthogonal Matching Pursuit problems using only\n",
      "        the Gram matrix X.T * X and the product X.T * y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <omp>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Gram : array, shape (n_features, n_features)\n",
      "            Gram matrix of the input data: X.T * X\n",
      "        \n",
      "        Xy : array, shape (n_features,) or (n_features, n_targets)\n",
      "            Input targets multiplied by X: X.T * y\n",
      "        \n",
      "        n_nonzero_coefs : int\n",
      "            Desired number of non-zero entries in the solution. If None (by\n",
      "            default) this value is set to 10% of n_features.\n",
      "        \n",
      "        tol : float\n",
      "            Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "        \n",
      "        norms_squared : array-like, shape (n_targets,)\n",
      "            Squared L2 norms of the lines of y. Required if tol is not None.\n",
      "        \n",
      "        copy_Gram : bool, optional\n",
      "            Whether the gram matrix must be copied by the algorithm. A false\n",
      "            value is only helpful if it is already Fortran-ordered, otherwise a\n",
      "            copy is made anyway.\n",
      "        \n",
      "        copy_Xy : bool, optional\n",
      "            Whether the covariance vector Xy must be copied by the algorithm.\n",
      "            If False, it may be overwritten.\n",
      "        \n",
      "        return_path : bool, optional. Default: False\n",
      "            Whether to return every value of the nonzero coefficients along the\n",
      "            forward path. Useful for cross-validation.\n",
      "        \n",
      "        return_n_iter : bool, optional default False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : array, shape (n_features,) or (n_features, n_targets)\n",
      "            Coefficients of the OMP solution. If `return_path=True`, this contains\n",
      "            the whole coefficient path. In this case its shape is\n",
      "            (n_features, n_features) or (n_features, n_targets, n_features) and\n",
      "            iterating over the last axis yields coefficients in increasing order\n",
      "            of active features.\n",
      "        \n",
      "        n_iters : array-like or int\n",
      "            Number of active features across every target. Returned only if\n",
      "            `return_n_iter` is set to True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        OrthogonalMatchingPursuit\n",
      "        orthogonal_mp\n",
      "        lars_path\n",
      "        decomposition.sparse_encode\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n",
      "        Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "        Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "        (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "        \n",
      "        This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "        M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "        Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "        https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "    \n",
      "    ridge_regression(X, y, alpha, *, sample_weight=None, solver='auto', max_iter=None, tol=0.001, verbose=0, random_state=None, return_n_iter=False, return_intercept=False, check_input=True)\n",
      "        Solve the ridge equation by the method of normal equations.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {ndarray, sparse matrix, LinearOperator} of shape         (n_samples, n_features)\n",
      "            Training data\n",
      "        \n",
      "        y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "            Target values\n",
      "        \n",
      "        alpha : float or array-like of shape (n_targets,)\n",
      "            Regularization strength; must be a positive float. Regularization\n",
      "            improves the conditioning of the problem and reduces the variance of\n",
      "            the estimates. Larger values specify stronger regularization.\n",
      "            Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "            :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "            :class:`sklearn.svm.LinearSVC`. If an array is passed, penalties are\n",
      "            assumed to be specific to the targets. Hence they must correspond in\n",
      "            number.\n",
      "        \n",
      "        sample_weight : float or array-like of shape (n_samples,), default=None\n",
      "            Individual weights for each sample. If given a float, every sample\n",
      "            will have the same weight. If sample_weight is not None and\n",
      "            solver='auto', the solver will be set to 'cholesky'.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\n",
      "            Solver to use in the computational routines:\n",
      "        \n",
      "            - 'auto' chooses the solver automatically based on the type of data.\n",
      "        \n",
      "            - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "              coefficients. More stable for singular matrices than 'cholesky'.\n",
      "        \n",
      "            - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "              obtain a closed-form solution via a Cholesky decomposition of\n",
      "              dot(X.T, X)\n",
      "        \n",
      "            - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "              scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "              more appropriate than 'cholesky' for large-scale data\n",
      "              (possibility to set `tol` and `max_iter`).\n",
      "        \n",
      "            - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "              scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      "              procedure.\n",
      "        \n",
      "            - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "              its improved, unbiased version named SAGA. Both methods also use an\n",
      "              iterative procedure, and are often faster than other solvers when\n",
      "              both n_samples and n_features are large. Note that 'sag' and\n",
      "              'saga' fast convergence is only guaranteed on features with\n",
      "              approximately the same scale. You can preprocess the data with a\n",
      "              scaler from sklearn.preprocessing.\n",
      "        \n",
      "        \n",
      "            All last five solvers support both dense and sparse data. However, only\n",
      "            'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is\n",
      "            True.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               Stochastic Average Gradient descent solver.\n",
      "            .. versionadded:: 0.19\n",
      "               SAGA solver.\n",
      "        \n",
      "        max_iter : int, default=None\n",
      "            Maximum number of iterations for conjugate gradient solver.\n",
      "            For the 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      "            by scipy.sparse.linalg. For 'sag' and saga solver, the default value is\n",
      "            1000.\n",
      "        \n",
      "        tol : float, default=1e-3\n",
      "            Precision of the solution.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Verbosity level. Setting verbose > 0 will display additional\n",
      "            information depending on the solver used.\n",
      "        \n",
      "        random_state : int, RandomState instance, default=None\n",
      "            Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      "            See :term:`Glossary <random_state>` for details.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            If True, the method also returns `n_iter`, the actual number of\n",
      "            iteration performed by the solver.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        return_intercept : bool, default=False\n",
      "            If True and if X is sparse, the method also returns the intercept,\n",
      "            and the solver is automatically changed to 'sag'. This is only a\n",
      "            temporary fix for fitting the intercept with sparse data. For dense\n",
      "            data, use sklearn.linear_model._preprocess_data before your regression.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        check_input : bool, default=True\n",
      "            If False, the input arrays X and y will not be checked.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "            Weight vector(s).\n",
      "        \n",
      "        n_iter : int, optional\n",
      "            The actual number of iteration performed by the solver.\n",
      "            Only returned if `return_n_iter` is True.\n",
      "        \n",
      "        intercept : float or ndarray of shape (n_targets,)\n",
      "            The intercept of the model. Only returned if `return_intercept`\n",
      "            is True and if X is a scipy sparse array.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function won't compute the intercept.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['ARDRegression', 'BayesianRidge', 'ElasticNet', 'ElasticNet...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\user\\anaconda3\\envs\\adp_code\\lib\\site-packages\\sklearn\\linear_model\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "help(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "499775ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ARDRegression in module sklearn.linear_model._bayes:\n",
      "\n",
      "class ARDRegression(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      " |  ARDRegression(*, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      " |  \n",
      " |  Bayesian ARD regression.\n",
      " |  \n",
      " |  Fit the weights of a regression model, using an ARD prior. The weights of\n",
      " |  the regression model are assumed to be in Gaussian distributions.\n",
      " |  Also estimate the parameters lambda (precisions of the distributions of the\n",
      " |  weights) and alpha (precision of the distribution of the noise).\n",
      " |  The estimation is done by an iterative procedures (Evidence Maximization)\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_iter : int, default=300\n",
      " |      Maximum number of iterations.\n",
      " |  \n",
      " |  tol : float, default=1e-3\n",
      " |      Stop the algorithm if w has converged.\n",
      " |  \n",
      " |  alpha_1 : float, default=1e-6\n",
      " |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      " |      over the alpha parameter.\n",
      " |  \n",
      " |  alpha_2 : float, default=1e-6\n",
      " |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      " |      Gamma distribution prior over the alpha parameter.\n",
      " |  \n",
      " |  lambda_1 : float, default=1e-6\n",
      " |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      " |      over the lambda parameter.\n",
      " |  \n",
      " |  lambda_2 : float, default=1e-6\n",
      " |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      " |      Gamma distribution prior over the lambda parameter.\n",
      " |  \n",
      " |  compute_score : bool, default=False\n",
      " |      If True, compute the objective function at each step of the model.\n",
      " |  \n",
      " |  threshold_lambda : float, default=10 000\n",
      " |      threshold for removing (pruning) weights with high precision from\n",
      " |      the computation.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be centered).\n",
      " |  \n",
      " |  normalize : bool, default=False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      " |      on an estimator with ``normalize=False``.\n",
      " |  \n",
      " |  copy_X : bool, default=True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  verbose : bool, default=False\n",
      " |      Verbose mode when fitting the model.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array-like of shape (n_features,)\n",
      " |      Coefficients of the regression model (mean of distribution)\n",
      " |  \n",
      " |  alpha_ : float\n",
      " |     estimated precision of the noise.\n",
      " |  \n",
      " |  lambda_ : array-like of shape (n_features,)\n",
      " |     estimated precisions of the weights.\n",
      " |  \n",
      " |  sigma_ : array-like of shape (n_features, n_features)\n",
      " |      estimated variance-covariance matrix of the weights\n",
      " |  \n",
      " |  scores_ : float\n",
      " |      if computed, value of the objective function (to be maximized)\n",
      " |  \n",
      " |  intercept_ : float\n",
      " |      Independent term in decision function. Set to 0.0 if\n",
      " |      ``fit_intercept = False``.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import linear_model\n",
      " |  >>> clf = linear_model.ARDRegression()\n",
      " |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      " |  ARDRegression()\n",
      " |  >>> clf.predict([[1, 1]])\n",
      " |  array([1.])\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  For an example, see :ref:`examples/linear_model/plot_ard.py\n",
      " |  <sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\n",
      " |  competition, ASHRAE Transactions, 1994.\n",
      " |  \n",
      " |  R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n",
      " |  http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n",
      " |  Their beta is our ``self.alpha_``\n",
      " |  Their alpha is our ``self.lambda_``\n",
      " |  ARD is a little different than the slide: only dimensions/features for\n",
      " |  which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\n",
      " |  discarded.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ARDRegression\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      sklearn.linear_model._base.LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the ARDRegression model according to the given training data\n",
      " |      and parameters.\n",
      " |      \n",
      " |      Iterative procedure to maximize the evidence\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples in the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values (integers). Will be cast to X's dtype if necessary\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  predict(self, X, return_std=False)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      In addition to the mean of the predictive distribution, also its\n",
      " |      standard deviation can be returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      return_std : bool, default=False\n",
      " |          Whether to return the standard deviation of posterior prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_mean : array-like of shape (n_samples,)\n",
      " |          Mean of predictive distribution of query points.\n",
      " |      \n",
      " |      y_std : array-like of shape (n_samples,)\n",
      " |          Standard deviation of predictive distribution of query points.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix or a list of generic objects instead,\n",
      " |          shape = (n_samples, n_samples_fitted),\n",
      " |          where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The R2 score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model._bayes import ARDRegression\n",
    "help(ARDRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce2eb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X= df[['height', 'weight']]\n",
    "y = df['waistline']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3715fe19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.53879393, -0.19168936])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ARDRegression(n_iter=1000, alpha_2=.005, lambda_1=.005, fit_intercept=False) # 부적절한 균일분포\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66cf646",
   "metadata": {},
   "source": [
    "#### (2) 위에서 만든 모델을 바탕으로 키 180cm, 허리둘레 85cm인 남성의 몸무게를 추정하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "501f5131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80.68931085])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[180, 85]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f5a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adp_code",
   "language": "python",
   "name": "adp_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
